{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: xxhash, multiprocess, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [datasets]2/3\u001b[0m [datasets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed datasets-3.6.0 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "ds = load_dataset(\"mutiyama/alt\", \"alt-parallel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SNT.URLID': '80188',\n",
       " 'SNT.URLID.SNTID': '1',\n",
       " 'url': 'http://en.wikinews.org/wiki/2007_Rugby_World_Cup:_Italy_31_-_5_Portugal',\n",
       " 'translation': {'bg': 'ফ্রান্সের প্যারিসের পার্ক দি প্রিন্সেস-এ হওয়া ২০০৭-এর রাগবি বিশ্বকাপের পুল সি-তে ইটালি পর্তুগালকে ৩১-৫ গোলে হারিয়েছে।',\n",
       "  'en': 'Italy have defeated Portugal 31-5 in Pool C of the 2007 Rugby World Cup at Parc des Princes, Paris, France.',\n",
       "  'en_tok': 'Italy have defeated Portugal 31-5 in Pool C of the 2007 Rugby World Cup at Parc des Princes , Paris , France .',\n",
       "  'fil': 'Natalo ng Italya ang Portugal sa puntos na 31-5 sa Grupong C noong 2007 sa Pandaigdigang laro ng Ragbi sa Parc des Princes, Paris, France.',\n",
       "  'hi': '2007 में फ़्रांस, पेरिस के पार्क डेस प्रिंसेस में हुए रग्बी विश्व कप के पूल C में इटली ने पुर्तगाल को 31-5 से हराया।',\n",
       "  'id': 'Italia berhasil mengalahkan Portugal 31-5 di grup C dalam Piala Dunia Rugby 2007 di Parc des Princes, Paris, Perancis.',\n",
       "  'ja': 'フランスのパリ、パルク・デ・プランスで行われた2007年ラグビーワールドカップのプールCで、イタリアは31対5でポルトガルを下した。',\n",
       "  'khm': 'អ៊ីតាលីបានឈ្នះលើព័រទុយហ្គាល់ 31-5 ក្នុងប៉ូលCនៃពីធីប្រកួតពានរង្វាន់ពិភពលោកនៃកីឡាបាល់ឱបឆ្នាំ2007ដែលប្រព្រឹត្តនៅប៉ាសឌេសប្រីន ក្រុងប៉ារីស បារាំង។',\n",
       "  'lo': 'ອິຕາລີໄດ້ເສຍໃຫ້ປ໊ອກຕຸຍການ 31 ຕໍ່ 5 ໃນພູລ C ຂອງ ການແຂ່ງຂັນຣັກບີ້ລະດັບໂລກປີ 2007 ທີ່ ປາກເດແພຣັງ ປາຣີ ປະເທດຝຣັ່ງ.',\n",
       "  'ms': 'Itali telah mengalahkan Portugal 31-5 dalam Pool C pada Piala Dunia Ragbi 2007 di Parc des Princes, Paris, Perancis.',\n",
       "  'my': 'ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂၀၀၇ခုနှစ် ရပ်ဘီ ကမ္ဘာ့ ဖလား တွင် အီတလီ သည် ပေါ်တူဂီ ကို ၃၁-၅ ဂိုး ဖြင့် ရေကူးကန် စီ တွင် ရှုံးနိမ့်သွားပါသည် ။',\n",
       "  'th': 'อิตาลีได้เอาชนะโปรตุเกสด้วยคะแนน31ต่อ5 ในกลุ่มc ของการแข่งขันรักบี้เวิลด์คัพปี2007 ที่สนามปาร์กเดแพร็งส์ ที่กรุงปารีส ประเทศฝรั่งเศส',\n",
       "  'vi': 'Ý đã đánh bại Bồ Đào Nha với tỉ số 31-5 ở Bảng C Giải vô địch Rugby thế giới 2007 tại Parc des Princes, Pari, Pháp.',\n",
       "  'zh': '意大利在法国巴黎王子公园体育场举办的2007年橄榄球世界杯C组以31-5击败葡萄牙。'}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this we can see that there are 4 features, denoting the id, url and translations. for our task of translation from burmese to english we only need the source, which is 'my' key in and the target which is 'en' key in translation. so we'll drop the rest of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train_dataset, test_dataset, val_dataset= clean_dataset(ds)\n",
    "def clean_dataset(dataset):\n",
    "    train_dataset = dataset['train']\n",
    "    test_dataset = dataset['test']\n",
    "    val_dataset = dataset['validation']\n",
    "    \n",
    "    # Create new dataset structure with id and translation dictionary\n",
    "    def restructure(example, idx):\n",
    "        return {\n",
    "            'id': idx,\n",
    "            'translation': {\n",
    "                'burmese': example['translation']['my'],\n",
    "                'english': example['translation']['en']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Apply restructuring with indices\n",
    "    train_dataset = train_dataset.map(\n",
    "        restructure, with_indices=True\n",
    "    ).remove_columns(['SNT.URLID', 'SNT.URLID.SNTID', 'url'])\n",
    "    \n",
    "    test_dataset = test_dataset.map(\n",
    "        restructure, with_indices=True\n",
    "    ).remove_columns([ 'SNT.URLID', 'SNT.URLID.SNTID', 'url'])\n",
    "    \n",
    "    val_dataset = val_dataset.map(\n",
    "        restructure, with_indices=True\n",
    "    ).remove_columns(['SNT.URLID', 'SNT.URLID.SNTID', 'url'])\n",
    "    \n",
    "    # Filter out rows with None values\n",
    "    train_dataset = train_dataset.filter(\n",
    "        lambda x: x['translation']['burmese'] is not None and x['translation']['english'] is not None\n",
    "    )\n",
    "    \n",
    "    test_dataset = test_dataset.filter(\n",
    "        lambda x: x['translation']['burmese'] is not None and x['translation']['english'] is not None\n",
    "    )\n",
    "    \n",
    "    val_dataset = val_dataset.filter(\n",
    "        lambda x: x['translation']['burmese'] is not None and x['translation']['english'] is not None\n",
    "    )\n",
    "    \n",
    "    return train_dataset, test_dataset, val_dataset\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = clean_dataset(ds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'burmese': 'ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂၀၀၇ခုနှစ် ရပ်ဘီ ကမ္ဘာ့ ဖလား တွင် အီတလီ သည် ပေါ်တူဂီ ကို ၃၁-၅ ဂိုး ဖြင့် ရေကူးကန် စီ တွင် ရှုံးနိမ့်သွားပါသည် ။', 'english': 'Italy have defeated Portugal 31-5 in Pool C of the 2007 Rugby World Cup at Parc des Princes, Paris, France.'}, 'id': 0}\n",
      "{'translation': {'burmese': 'ဆစ်ဒနီ က ရန့်ဝစ်(ခ်) မြင်းပြိုင်ကွင်း မှ မျိုးသန့် ပြိုင်မြင်း ရှစ်ကောင် ဟာ မြင်းတုတ်ကွေးရောဂါ ကူးစက်ခံခဲ့ရတယ် ဆိုတာ အတည်ပြုခဲ့ပါတယ် ။', 'english': 'It has been confirmed that eight thoroughbred race horses at Randwick Racecourse in Sydney have been infected with equine influenza.'}, 'id': 0}\n",
      "{'translation': {'burmese': '\" သူ၏ ဆုံးပါးခြင်း အတွက် ကျွန်တော်တို့ ဝမ်းနည်း သော်ငြားလည်း ၊ လူမျိုးရေး နှင့် ဘာသာရေး ရန်စွယ် ကို နှိုးဆွပေး သော အမွေနှစ် တစ်ခု ကို သူ ချန်ထားခဲ့သည် ။ \"', 'english': '\"Though we are sad for his loss, he left a legacy that will inflame the enemy nation and religion.\"'}, 'id': 0}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(test_dataset[0])\n",
    "print(val_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'burmese': 'အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု အဖြစ် စတုတ္ထ မိနစ် တွင် အမှတ်ပေးခြင်း ကို ဖွင့်လှစ်ပေးခဲ့သည် ။',\n",
       " 'english': 'Andrea Masi opened the scoring in the fourth minute with a try for Italy.'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class InputEmbeddings(nn.Module): #dmodel of 256 should be okay for this model\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embedding=nn.Embedding(vocab_size,d_model)\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we will be using character level tokenization for burmese to make things easier. we are going to choose the dmodel as 256 which means each character will be represented as a vector of size 256. if your sentence has 20 characters then you'll have 20 vectors each of size 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the paper we are multiplying the embedding vectors by sqrt(dmodel) as mentioned in the paper. It helps maintain proper variance in the network, improves gradient flow during back propagation.This scaling factor might seem minor, but it's crucial for stable training. Networks without this scaling often train slower or fail to converge properly, especially as you increase the model depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model: int, seq_len: int, dropout: float) -> None: \n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.seq_len=seq_len\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "        #create a matrix of shape(seq_len, d_model) each vector is of the size d_model\n",
    "        pe=torch.zeros(seq_len,d_model)\n",
    "        position=torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1) # vector of shape (seq_len,1)\n",
    "        div_term= torch.exp(torch.arange(0,d_model,2).float() * (-math.log(10000.0) / d_model)) #calculating in log space for numerical stability\n",
    "        pe[:,0::2]=torch.sin(position * div_term)\n",
    "        pe[:,1::2]=torch.cos(position * div_term)\n",
    "\n",
    "        pe=pe.unsqueeze(0) #(1,seq_len,d_model) to add batch dimension\n",
    "        self.register_buffer('pe',pe) #register buffer is used to register the pe matrix as a buffer, it is not a parameter of the model and is not updated during backpropagation\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=x+self.pe[:,:x.shape[1],:].requires_grad_(False) #(batch_size,seq_len,d_model)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequence length is the maximum length a sentence can have, dropout is used to reduce overfitting. we are creating a matrix of size sequence length and dmodel for the positional encoding. the positional encoding is used to understand the word order in sentences which is crucial in understanding the context. so we add a unique position fingerprint to each token embedding. the positional encoding is not a learned parameter its only computed once and its stored in the model memory. we then apply a dropout to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features: int, eps: float = 10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps=eps #epsilon is used to avoid division by zero and to ensure numerical stability\n",
    "        self.alpha=nn.Parameter(torch.ones(features)) #multiplicative factor\n",
    "        self.bias=nn.Parameter(torch.zeros(features)) #additive factor\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean=x.mean(dim=-1,keepdim=True) #mean of the last dimension\n",
    "        std= x.std(dim=-1,keepdim=True)\n",
    "        return self.alpha * (x-mean)/(std+self.eps) + self.bias #layer normalization formula from the paper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer normaliztion normalises each token's feature vector independently making mean =0 and variance =1 we use the formula given in the paper to do this. alpha and beta helps the model to tweak the mean and variance as required otherwise it would be forced to work with mean = 0 and variance =1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float)-> None:\n",
    "        super().__init__()\n",
    "        self.linear_1=nn.Linear(d_model,d_ff) #shape of the w1 matrix is d_ff,d_model pytorch creates a weight matrix of shape (out_features,in_features) and adds bias automatically\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.linear_2=nn.Linear(d_ff,d_model)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the ffn helps to introduce non linearity and increases the models learing capacity by expanding the dimension. attention determines which token to focus on while ffn determines what to do with the aggregate information. nn.linear applies a linear transformation y=xW^T + b. so in our case d_model is of 256 and d_ff is usually 4 times d_model so it becomes 1024. now it creates a matrix of shape 256,1024 and  our input is of the size [batch_sie, sequence_length, d_model]. so the output of linear1 will be of the shape [batch_size, sequence_length, d_ff] (eg: [1,256]*[256,1024] we take the transpose of w so the  output will be [1,1024]) now we apply dropout on it to randomly set elements to zero with probability p. then we apply linear 2 transformation which is of the shape [256,1024] eg([1,1024] multiplied by transpose of [256,1024], gives the output of size [1,256] )\n",
    "so we get our original dimension back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float)-> None:\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.h=h\n",
    "        self.d_k=d_model//h #we have to make sure that d_model is divisible by h. h is the number of heads.\n",
    "        self.w_q=nn.Linear(d_model,d_model)\n",
    "        self.w_k=nn.Linear(d_model,d_model)\n",
    "        self.w_v=nn.Linear(d_model,d_model)\n",
    "        self.w_o=nn.Linear(d_model,d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query,key,value,mask,dropout: nn.Module):\n",
    "        d_k=query.shape[-1]\n",
    "\n",
    "\n",
    "        attention_scores=(query @ key.transpose(-2,-1))/math.sqrt(d_k)  #(batch_size,h,seq_len,d_k) --> (bathc_size,h,seq_len,seq_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask==0,-1e9)\n",
    "        attention_scores=attention_scores.softmax(dim=-1) #(batch_size,h,seq_len,seq_len)\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_scores=dropout(attention_scores)\n",
    "        return (attention_scores @ value), attention_scores #attention_scores for visualization\n",
    "    \n",
    "    def forward(self,q,k,v,mask): #we use mask basically to avoid specific tokens interacting with each other. we do this before applying softmax\n",
    "        query=self.w_q(q) #(batch_size,seq_len,d_model) --> (batch_size,seq_len,d_model)\n",
    "        key = self.w_k(k) #(batch_size,seq_len,d_model) --> (batch_size,seq_len,d_model)\n",
    "        value = self.w_v(v) #(batch_size,seq_len,d_model) --> (batch_size,seq_len,d_model)\n",
    "\n",
    "        query=query.view(query.shape[0],query.shape[1],self.h,self.d_k).transpose(1,2) #(batch_size,seq_len,d_model) --> (batch_size,seq_len,h,d_k) --> (batch_size, h , seq_len, d_k)\n",
    "        key=key.view(key.shape[0],key.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        value=value.view(value.shape[0],value.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "\n",
    "        # (batch_size, h , seq_len, d_k) --> (batch_size, seq_len, h, d_k) --> (batch_size, seq_len, d_model)\n",
    "        x, self.attention_scores=MultiHeadAttentionBlock.attention(query,key,value,mask,self.dropout)\n",
    "        x=x.transpose(1,2).contiguous().view(x.shape[0],-1,self.h * self.d_k) #h*dk=d_model because d_k = d_model/h\n",
    "        # (batch_size, seq_len, d_model) --> (batch_size, seq_len, d_model)\n",
    "        return self.w_o(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first the input matrix of shape (seq,d_model) is split into 3 matrices of shape(seq,d_model) called query,key,value and we apply a linear transformation using the linear layer on each of them of shape(d_model,d_model) to get the output matrices Q', K' and V' of shape(seq,d_model), which was our input shape as well. We then choose the number of heads, the d_model should be divisible by number of heads. the value used in the paper was 8 but to keep the model use less resources we'll keep it to 4 in our implementation. now we use the view function to reshape te tensor without changing the data. we split the embedding dimension according to the number of heads which is a vector called dk of seq length which is defined by dmodel//h.we then transpose each of the q,k,v matrices in dimensions(1,2) i'e we swap number of heads and sequence length, so that each head gets access to the entire sequence allowing the heads to process the enire sequence length independently and in parallel we then calculate the attention scores by matrix multiplying query with the transpose of key(-2,-1) i.e we swap the last two dimensions. query_shape:[batch_size, h , seq_len, dk], key shape: [batch_size, h, seq_len, shape], key_transpose: [batch_size,h,d_k,seq_len] we divide it by square root of d_k the resulting matrix we get is of the shape [batch_size, h, seq_len, seq_len]. This shape represents attention scores between each pair of token, ie characters in our case. each cell[i,j] represents how much character i should attend to character j. acharacter might strongly attend to distant characters that complete its meaning. characters forming a single logical unit like a word will have strong mututal attention. the model kind of learns which character relationships matter without expliscit linguistic rules. the attention mechanism kind of automatically discovers meaningful relationships between characters even when they are separated by many positions and . Then we apply the mask function to  prevent certain connections between tokens. for ecample to mask padding token to prevent attenting to padding tokens it sets certain positions to -infinity before passing to the softmax so that their probability is effectively zero, so they get no attention then we pass it through a softmax and matrix multiply it with value matrix and return it.the attention scores represet how much each token should pay attention to every other token. For example, in the sentence \"you are lovely\" the token \"lovey\" might atted to token \"you\" and \"are\" because to understand who is lovely and how.higher scores means stronger relation ship. I think the magic of transformers lie in this mechanism. how its able to encode these data into an effective vector space and to succesfully embed the context into that vector space sounds very fascinating to me. I wonder how they came up with this.  then we concatinate each of the heads along embedding dimension it goes from [batch_size, seq_len,h*d_K] to batch_size,seq_len,dmodel] because dk=dmodel/h.so the linear transformation of w0 unifies diverese information from different heads and combine it into a unified embedding space as the input so that the model can carry on with its execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "\n",
    "    def __init__(self,features: int,dropout: float)-> None:\n",
    "        super().__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.norm=LayerNormalization(features)\n",
    "\n",
    "    def forward(self,x,sublayer: nn.Module):\n",
    "        return x+self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "   \n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So residual layer is a type of connection that is used the input of a layer back to its output. \n",
    "output= x + layer(x), where layer is the transformation applied (either multi head attention or feed forward network). in deep neural networks gradients can either vanish or explode, which makes the training unstable. so they kind of solve that problem by making the output  a blend of the original input and the transformed version. even if multihead is poorly initialized it still has access to its original signal. the original paper uses a post-layer norm which basically means applying LayerNormalization after applyin the sublayer and adding the output back to the sublayer. But it has been found that prelayer norm leads to more stable training because inputs to the sublayer are normalized avoiding explosion/vanishing activations. so we are going to use pre norm here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,features: int,self_attention_block: MultiHeadAttentionBlock,feed_forward_block: FeedForwardBlock,dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block=self_attention_block\n",
    "        self.feed_forward_block=feed_forward_block\n",
    "        self.residual_connections=nn.ModuleList([ResidualConnection(features,dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self,x,src_mask):\n",
    "        x=self.residual_connections[0](x,lambda x: self.self_attention_block(x,x,x,src_mask))\n",
    "        x=self.residual_connections[1](x,lambda x: self.feed_forward_block(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can have 'n' number of encoder blocks sequentially. each block processes the output of the previous block. each encoder block will have a multihead self attention sublayer, a feed forward sublayer and two residual connections one for each sublayers. the first residual connection normalises the input x, applies self attention with query, key and value all set to x. it then applies the dropout and adds the result back to original x. the second residual connection normalises the output from the previous step applies the feed forward network, applie dropout and adds the result back to the input of this sublayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features: int, layers: nn.ModuleList)-> None:\n",
    "        super().__init__()\n",
    "        self.layers=layers #list of encoder blocks\n",
    "        self.norm=LayerNormalization(features)\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Encoder contains multiple encoder blocks each containing a self attention mechanism, feed-forward network and residual connections stacked sequentially and then is finally passed through a layer normalization the original paper uses 6 encoder blocks stacked together in our case, i think 2-4 layers will be enough considering our hardware limits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,features: int,self_attention_block: MultiHeadAttentionBlock,cross_attention_block: MultiHeadAttentionBlock,feed_forward_block: FeedForwardBlock,dropout: float)-> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block=self_attention_block\n",
    "        self.cross_attention_block=cross_attention_block\n",
    "        self.feed_forward_block=feed_forward_block\n",
    "        self.residual_connections=nn.ModuleList(ResidualConnection(features,dropout) for _ in range(3))\n",
    "\n",
    "    def forward(self,x,encoder_output, src_mask, tgt_mask): #src mask and target mask because we are dealing with translation here and they have different sequences\n",
    "\n",
    "        x=self.residual_connections[0](x,lambda x: self.self_attention_block(x,x,x,tgt_mask)) #mask of the decoder\n",
    "        x=self.residual_connections[1](x,lambda x: self.cross_attention_block(x, encoder_output, encoder_output,src_mask)) #query from decoder, key and value from encoder hence the cross attention and the mask of the encoder\n",
    "        x=self.residual_connections[2](x,lambda x: self.feed_forward_block(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the decoder block we use the same positional embedding and the input embedding layers with just different parameters i guess. the vocab size, the sequence length would be different since we are dealing with two completely different languages. it has 3 residual connections, the first one adds the input to the output of self attention. the next residual connection combines the output of the previous self attention and applies cross attention to it with the current output as query encoder output as key and value. This is where the decoder kind of learns what parts of the source sentence to focus on when generating each target word. the third residual connection is to combine the output of the cross attention block with the feed forward blocks output. The decoder is made up of n decoder blocks. The source mask is used both in encoder and cross attention. it tells the model which tokens to ignore such as padding tokens. target mask is only used in the decoder self attention. this has two purposes one is to hide padding tokens and the other one is to prevent the decoder from seing the future tokens. during training we feed the entire target sequence into the decoder but the tokens can attend to only what it has seen so far so it essentially masks out the future tokens this kind of helps the decoder to learn the underlying function of input space and predict the next token based only on what it has seen so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,features: int,layers: nn.ModuleList)-> None:\n",
    "        super().__init__()\n",
    "        self.layers=layers #list of decoder blocks\n",
    "        self.norm=LayerNormalization(features)\n",
    "    \n",
    "    def forward(self,x,encoder_output,src_mask,tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,encoder_output,src_mask,tgt_mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can have n number of decoder blocks arranged sequentially one after the other, each has a self attention sublayer, cross attention sublayer a feed forward sublayer, three residual connections(one for each sublayer) and normalization to the final output. th number of decoder = number of encoders (typically works well, but can be experimented with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module): #this is the final layer of the decoder that projects the output of the decoder which is of shape (batch_size,seq_len,d_model) to (batch_size,seq_len,vocab_size) where vocab size is the size of the target vocabulary\n",
    "    def __init__(self,d_model,vocab_size)-> None:\n",
    "        super().__init__()\n",
    "        self.proj=nn.Linear(d_model,vocab_size) #linear transformation to project the output of the decoder to the target vocabulary\n",
    "\n",
    "    def forward(self,x):\n",
    "        return torch.log_softmax(self.proj(x),dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projection layer helps us to convert the output of the transformer from batch_size, seq_len, d_model to batch_size, seq_len, vocab_size eseentially converting the output to an element in the target vocabulary. so we calculate the projection of the last dimension which is d_model and apply log_softmax for numerical stabilty which gives us the probability distribution of each token across the vocab size ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embedding: InputEmbeddings, tgt_embedding: InputEmbeddings, src_pos_embedding: PositionalEncoding, tgt_pos_embedding: PositionalEncoding,projection_layer: ProjectionLayer)-> None:\n",
    "        super().__init__()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        self.src_embedding=src_embedding\n",
    "        self.tgt_embedding=tgt_embedding\n",
    "        self.src_pos_embedding=src_pos_embedding\n",
    "        self.tgt_pos_embedding=tgt_pos_embedding\n",
    "        self.projection_layer=projection_layer\n",
    "        \n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src=self.src_embedding(src)\n",
    "        src=self.src_pos_embedding(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "        tgt=self.tgt_embedding(tgt)\n",
    "        tgt=self.tgt_pos_embedding(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self,x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so this class defines the structure of a transformer and initializes a transformer object. we will have to specify its encoder, decoder, src_embeddings, target embeddings, position vectors of source and target embeddings, and the projection layer finally converts the transformer output to the vocabulary size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=256, N: int=4, h: int =4, dropout: float=0.1, d_ff: int=1024)-> Transformer: #in the paper d_model=512, N=6, h=8,d_ff=2048\n",
    "    \n",
    "    src_embedding=InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embedding=InputEmbeddings(d_model,tgt_vocab_size)\n",
    "    src_pos_embedding=PositionalEncoding(d_model,src_seq_len,dropout)\n",
    "    tgt_pos_embedding=PositionalEncoding(d_model,tgt_seq_len, dropout)\n",
    "\n",
    "    encoder_blocks=[]\n",
    "    for i in range(N):\n",
    "        encoder_self_attention_block=MultiHeadAttentionBlock(d_model,h,dropout)\n",
    "        feed_forward_block=FeedForwardBlock(d_model,d_ff,dropout)\n",
    "        encoder_block=EncoderBlock(d_model,encoder_self_attention_block,feed_forward_block,dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    decoder_blocks=[]\n",
    "    for i in range(N):\n",
    "        decoder_self_attention_block=MultiHeadAttentionBlock(d_model,h,dropout)\n",
    "        decoder_cross_attention_block=MultiHeadAttentionBlock(d_model,h,dropout)\n",
    "        feed_forward_block=FeedForwardBlock(d_model,d_ff,dropout)\n",
    "        decoder_block=DecoderBlock(d_model,decoder_self_attention_block,decoder_cross_attention_block,feed_forward_block,dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    encoder=Encoder(d_model,nn.ModuleList(encoder_blocks))\n",
    "    decoder=Decoder(d_model,nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    projection_layer=ProjectionLayer(d_model,tgt_vocab_size)\n",
    "\n",
    "    transformer=Transformer(encoder,decoder,src_embedding,tgt_embedding,src_pos_embedding,tgt_pos_embedding,projection_layer)\n",
    "\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim()<2: continue\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function helps us to create a transformer with the given intial parameters.we first define the input and positional embeddings then create N encoder blocks which contains self attention, feed forward sublayers and residual connections. decoder blocks have cross attention as well as well as all the other elements an encoder block has. we then create the encoder blocks and decoder blocks by passing the appropriate parameters and then create a module list, then we define the projection layer finally we create a transformer object using these parts we created. we then use xavier uniform weight initialization. this is because without proper intialization networks struggle to learn effectively "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since burmese doesnt have clear word boundaries we are going to use character level tokenizer for burmese and word level tokenizer for english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace, CharDelimiterSplit, PreTokenizer, Split\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "def create_english_tokenizer(dataset,config):\n",
    "    tokenizer_path=Path(config['tokenizer_file'].format('en'))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer=Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "        tokenizer.pre_tokenizer=Whitespace()\n",
    "        trainer=WordLevelTrainer(special_tokens=['[PAD]','[SOS]','[EOS]','[UNK]'],min_frequency=2,vocab_size=10000)\n",
    "        english_texts = (item['translation']['english'] for item in dataset)\n",
    "        tokenizer.train_from_iterator(english_texts,trainer,length=len(dataset))\n",
    "        tokenizer.save('tokenizer_en')\n",
    "    else:\n",
    "        tokenizer=Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    " \n",
    "def create_burmese_tokenizer(dataset,config):\n",
    "    tokenizer_path=Path(config['tokenizer_file'].format('my'))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer=Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "        # tokenizer.pre_tokenizer=CharDelimiterSplit(delimiter='')\n",
    "        # tokenizer.pre_tokenizer=Character()\n",
    "        tokenizer.pre_tokenizer = Split(pattern=\"\", behavior=\"isolated\")\n",
    "        trainer=WordLevelTrainer(special_tokens=['[PAD]','[SOS]','[EOS]','[UNK]'],min_frequency=2,vocab_size=2000)\n",
    "        burmese_texts = (item['translation']['burmese'] for item in dataset)\n",
    "        tokenizer.train_from_iterator(burmese_texts,trainer,length=len(dataset))\n",
    "        tokenizer.save('tokenizer_my')\n",
    "    else:\n",
    "        tokenizer=Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "    def __init__(self, dataset, src_tokenizer, tgt_tokenizer, src_lang, tgt_lang, src_seq_len, tgt_seq_len)-> None:\n",
    "        super().__init__()\n",
    "        self.dataset=dataset\n",
    "        self.src_tokenizer=src_tokenizer\n",
    "        self.tgt_tokenizer=tgt_tokenizer\n",
    "        self.src_lang=src_lang\n",
    "        self.tgt_lang=tgt_lang\n",
    "        self.src_seq_len=src_seq_len\n",
    "        self.tgt_seq_len=tgt_seq_len\n",
    "\n",
    "        self.sos_token=src_tokenizer.token_to_id('[SOS]')\n",
    "        self.eos_token=src_tokenizer.token_to_id('[EOS]')\n",
    "        self.pad_token=src_tokenizer.token_to_id('[PAD]')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        src_tgt_pair=self.dataset[idx]\n",
    "        src_text=src_tgt_pair['translation']['burmese']\n",
    "        tgt_text=src_tgt_pair['translation']['english']\n",
    "\n",
    "        enc_input_tokens=self.src_tokenizer.encode(src_text).ids\n",
    "        dec_input_tokens=self.tgt_tokenizer.encode(tgt_text).ids\n",
    "\n",
    "  \n",
    "        \n",
    "        enc_num_padding_tokens=self.src_seq_len - len(enc_input_tokens) - 2 # subtracting 2 because of the sos and eos tokens\n",
    "        dec_num_padding_tokens=self.tgt_seq_len - len(dec_input_tokens) - 1 # why -1 here?\n",
    "        \n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            # print(f\"Warning: Sentence at index {idx} is too long. Source length: {len(enc_input_tokens)}, Target length: {len(dec_input_tokens)}\")\n",
    "            # Truncate if necessary\n",
    "            enc_input_tokens = enc_input_tokens[:self.src_seq_len - 2]  # Leave room for SOS and EOS\n",
    "            dec_input_tokens = dec_input_tokens[:self.tgt_seq_len - 1]  # Leave room for SOS\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        # we combine the sos, enc_input_tokens, eos and padding tokens to form the encoder input\n",
    "        encoder_input= torch.cat(\n",
    "            [\n",
    "                torch.tensor([self.sos_token],dtype=torch.int64),\n",
    "                torch.tensor(enc_input_tokens,dtype=torch.int64),\n",
    "                torch.tensor([self.eos_token],dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token]*enc_num_padding_tokens,dtype=torch.int64)\n",
    "\n",
    "            ],dim=0\n",
    "        )\n",
    "\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                torch.tensor([self.sos_token],dtype=torch.int64),\n",
    "                torch.tensor(dec_input_tokens,dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token]*dec_num_padding_tokens,dtype=torch.int64)\n",
    "            ],dim=0\n",
    "        )\n",
    "\n",
    "        label=torch.cat(\n",
    "            [\n",
    "\n",
    "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "            torch.tensor([self.eos_token],dtype=torch.int64),\n",
    "            torch.tensor([self.pad_token]*dec_num_padding_tokens,dtype=torch.int64)\n",
    "            ],dim=0\n",
    "            \n",
    "        )\n",
    "\n",
    "        assert encoder_input.size(0)==self.src_seq_len\n",
    "        assert decoder_input.size(0)==self.tgt_seq_len\n",
    "        assert label.size(0)==self.tgt_seq_len\n",
    "\n",
    "        return {\n",
    "            'encoder_input': encoder_input,\n",
    "            'decoder_input': decoder_input,\n",
    "            'encoder_mask': (encoder_input!=self.pad_token).unsqueeze(0).unsqueeze(0).int(), #(1,1,seq_len) how? explain what does unsqueeze do\n",
    "            'decoder_mask': (decoder_input!=self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
    "            'label': label, #(seq_len)\n",
    "            'src_text': src_text,\n",
    "            'tgt_text': tgt_text\n",
    "        }\n",
    "\n",
    "def causal_mask(size):\n",
    "    mask=torch.triu(torch.ones(1,size,size),diagonal=1).type(torch.int) # i get that we are setting the attention score of all the tokens after it to zeor right? but i dont really get the mask==0 part\n",
    "    return mask ==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so here we are ceating a Dataset class which we will use to train the model, it takes in the src language, trget language, its tokenizers and converts the sentence pairs in to their respective tokens. to be exact it takes in a sentence pair prepares the input for the encoder by concatenating the start of sentence token with the encoder input and adds the end of sentence token and finally adds the required amount of padding tokens to reach the fixed sequence length of the target. if my source and target sequence lengths are different where should i make those changes? then we create the decoder input which is a concatenation of start of sentence token, decoder input and the padding tokens. then we also create a label which is the actual output that the decoder is supposed to figure out. this labels are used in the loss function  then we also apply the binary and operatiion to apply the causal mask. so what the causal mask does is that, its the decoder's job to predict what comes next given the current, so it should not see the tokens infront of it. so the causal mask masks all the values in the attention output above the diagonal so each token only gets access to what it has seen. the triu function creates a matrix with 1s above the diagonal and we want to block those so we invert it with ==0. so essentially the & operation with the padding masks keeps only those positions that are not paddind and positions that are not future tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 4,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 1e-4, #optimal learning rate as sugggested in the video\n",
    "        \"src_lang\": \"burmese\",\n",
    "        \"tgt_lang\": \"english\",\n",
    "        \"src_seq_len\": 500, #95th percentile from training dataset, character tokenizer\n",
    "        \"tgt_seq_len\": 55, #95th percentile from training dataset, word tokenizer\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel1\",\n",
    "        \"ds_name\": \"mutiyama/alt\",\n",
    "        \"ds_subset\": \"alt-parallel\",\n",
    "        \"d_model\": 256,\n",
    "        \"N\": 6,\n",
    "        \"h\": 4,\n",
    "        \"d_ff\": 1024,\n",
    "        \"dropout\": 0.1,\n",
    "        \"preload\": None,\n",
    "    }\n",
    "def get_weights_file_path(config,epoch):\n",
    "    model_folder=config['model_folder']\n",
    "    model_basename=config['model_basename']\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
    "    return str(Path('.')/model_folder / model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Maximum length of source sentence: 826\n",
      "Maximum length of target sentence: 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  0: 100%|████████████████████████████████████████████████████████████████████████████████████| 4521/4521 [29:29<00:00,  2.56it/s, loss=5.571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: သောကြာ နေ့ တွင် ၊ အများဆုံး လူထု အသံလွင့် ဝန်ဆောင်မှု ဌာန နှင့်အတူ ၊ အေဘီစီ ၊ စီဘီအက်စ် ၊ အက်ဖ်အိုအိပ်စ် ၊ အန်န်ဘီစီ ၊ ဒဗလျူဘီ ၊ နှင့် ယူပီအန်န် စသော ၊ အဓိက အမေရိကန် ရုပ်မြင်သံကြား ကွန်ယက် ကြီး ခြောက် ခု လုံး သည် ပင်လယ်ကွေ့ ကမ်းရိုးတန်း အတွက် ပြပွဲ တစ် ခု ဖြစ်သော ၊ မုန်တိုင်း ရန် မှ ခိုလှုံရာ ဆိုသည့် တေးသီချင်း ကို လွင့် ရန် တစ် နာရီ စာ လှူဒါန်းပွဲ လေး တစ် ခု ကို ထူးခြားသော ပွဲ လေး တစ် ခု မှာ ၊ ပေါင်းစည်း ပြုလုပ်ခဲ့ကြသည် ။\n",
      "TARGET: On Friday, all six major American television networks; ABC, CBS, FOX, NBC, WB, and UPN, along with most PBS stations, united in a rare show of solidarity to air a one hour charity concert called Shelter from the Storm: A Concert for the Gulf Coast.\n",
      "PREDICTED: The , the , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ယခင် သြစတြေးလျ နိုင်ငံ ကိုယ်စားပြု အားကစား အသင်း အဖွဲ့ ဝင် များ အပါအဝင် အမျိုးမျိုးသော စွမ်းရည် အဆင့် ရှိ စက်ဘီး စီး ကျွမ်းကျင်သူ များ များစွာ ရှိသည် ၊ သို့သော် အပျော်တမ်း စက်ဘီး စီးသူ များ ကို အလွန် များပြားစွာ ထင်ထင်ရှားရှား တွေ့နိုင်သည် ။\n",
      "TARGET: There were a variety of cyclists of different skill levels including former Australian national team members, but casual cyclists were very much in evidence.\n",
      "PREDICTED: The is been a of the , and the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ဂျော်ဒန် ဘုရင် ၊ ဘုရင် အဘက်ဒူလာ သည် ၊ သူ၏ လေး နှစ် သက်တမ်း အားဖြင့် ၊ သက်တမ်းတစ်ဝက် အင်္ဂါနေ့ တွင် ၊ နိုင်ငံ ၏ ပါလီမန်လွှတ်တော် ကို ဖျက်သိမ်းခဲ့ပြီး စောစီးစႍာ ကျင်းပ မည့် ၊ အထွေထွေ ရွေးကောက်ပွဲ အတွက် အချိန်ဇယား ၏ မျှော်မှန်းချိန်ထက် နှစ် နှစ် စောလျှက် ခေါ်ခဲ့သည် ။\n",
      "TARGET: The King of Jordan, King Abdullah, dissolved the country's parliament on Tuesday, half-way through its four year term, and called for early general elections to be held, up to two years ahead of schedule.\n",
      "PREDICTED: The , the of the , , , , , , , , , , , .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: လော့စ် အင်ဂျလိ ဂလက်စီ သည် အစောပိုင်း ပထမပိုင်း တစ်ဝက် တွင် တစ် ဂိုး မျှသာ ရရှိခဲ့သည် ။\n",
      "TARGET: Los Angeles Galaxy recorded just a single shot on goal which came early in the 1st half.\n",
      "PREDICTED: The is been been been been been been been been been been been been been been been been been been been been .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: အာမေးနီးယား ဖိနပ် သည် သေချာစွာ ထိန်းသိမ်းထားသော အခြေအနေ တစ်ခု မှာ ရှိ ပြီး ဥရောပ ၌ ယခုတိုင် တွေ့ရသော ရှေးအကျဆုံးသော ဖိနပ် နှင့် ၊ ၄င်း ၏ ကမ္ဘာ မှာ သားရေ ဖိနပ် ၏ ရှေးအကျဆုံးသော အပိုင်း ကို ပြုလုပ်သော ၊ အော်ဇီ ရေခဲခေတ်လူသား များ မှာ တွေ့ရှိရသော တစ်ခု ထက် ပို ရှေးကျသော နှစ် တစ်ရာ လောက် က ဖြစ်ပါတယ် ။\n",
      "TARGET: The Armenian shoe is in a perfectly preserved condition and is a few hundred years older than the one found on Ötzi the Iceman, making it the oldest piece of leather footwear in the world, and the oldest footwear yet found in Europe.\n",
      "PREDICTED: The , the of the of the , and the .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  1: 100%|████████████████████████████████████████████████████████████████████████████████████| 4521/4521 [21:41<00:00,  3.47it/s, loss=6.094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ရဲ သတင်းဌာန မှ သတင်းပေးပို့ချက် အရ ၊ ရုရှား နိုက်ကလပ် တစ်ခု ၌ မနေ့က ပေါက်ကွဲမှု တစ်ရပ် သည် လူ အနည်းဆုံး ၉၄ ဦး သေဆုံးခဲ့ ပြီး နောက်ထပ် ၁၃၉ ဦး ဒဏ်ရာရရှိခဲ့သည် ။\n",
      "TARGET: An explosion yesterday in a Russian nightclub killed at least 94 people and injured a further 139, according to police reports.\n",
      "PREDICTED: The , the of the , was killed in the of the of the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ဝန်ကြီးချုပ် သည် ကျွန်းများ အား ဆန့်ကျင်တုံ့ပြန်မှု တစ်စုံတစ်ရာ မရှိစေ ရန် အုပ်ချုပ်ခဲ့ ပြီး ၊ ၎င်း သည် အနာဂတ် ငွေကြေး ဆိုင်ရာ ဘယ်လို ကူညီမှု မဆို ထိခိုက် လိမ့်မည် မဟုတ် ဟု ဆိုသည် ။\n",
      "TARGET: The Prime Minister has ruled out any retaliation against the islands, saying it would not affect any future aid funding.\n",
      "PREDICTED: The is a of the , and the , and the of the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: အိန္ဒိယ နိုင်ငံ ၏ အနောက်ဘက် ဆင်းဂေါလ် ပြည်နယ် ၊ ၏ မြို့တော် ၊ ခိုခတ္တား တွင် တစ် နာရီ လျှင် ၁၀၀ ကီလိုမီတာ အထိ လေပြင်း တိုက်ခတ် သဖြင့် သစ်ပင်များ အမြစ် မှ ကျွတ်ထွက် ပြီး ဆက်သွယ်ရေး လမ်းကြောင်း များ ပျက်စီး သွား ခဲ့သည် ။\n",
      "TARGET: Winds reaching 100 kilometres per hour in Kolkata, the capital of India's West Bengal province, have uprooted trees and destroyed communication lines.\n",
      "PREDICTED: The of the , the of the , , , , and the of the of the of the of the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ပက်ကာ သည် လောင်းကစား နှင့် ခရီးသွား စက်မှုလုပ်ငန်းများ တွင် လည်း ပါဝင်ပတ်သက်ခဲ့ ပြီး မဲလ်ဘုန်း ၌ ခရောင်း ကာစီနို ကို ပိုင်ဆိုင်ခဲ့သည် ။\n",
      "TARGET: Packer was also involved in the gambling and tourism industries and owned the Crown Casino in Melbourne.\n",
      "PREDICTED: The was a of the and .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ဘယ်ဘက် အဓိက စက် သာ လည်ပတ် နေ စဉ် ၊ ပြင်းထန်သော လှိုင်း များ သည် သင်္ဘော ကို ည ၇ နာရီ ကျော်ကျော် ထိ တိုင်အောင် ပြင်းထန်စွာ ရိုက်ခတ် ခဲ့သည် အင်္ဂါ နေ့ ည တွင် ၊ သင်္ဘော သည် ရှေးဟောင်း မြို့ တွင် ပင်လယ် ရေ အောက် သို့ နစ်မြုပ် သွား ခဲ့ သည် ။\n",
      "TARGET: While running on only the port main engine, strong waves battered the ship until shortly after 7:00 p.m. Tuesday night, when she went down at Antique.\n",
      "PREDICTED: The , the , said that the , was a , and that the of the , was , and the of the .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  2: 100%|████████████████████████████████████████████████████████████████████████████████████| 4521/4521 [21:22<00:00,  3.53it/s, loss=5.954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: သီချင်းစာအုပ်များ နှင့် ခက်ခဲစွာ စုဆောင်းထားသော မနုသဗေဒပညာများ ကို လက်ရှိ ဒစ်ဂျစ်တယ်ပြုလုပ်ထားသော အောက်စဖို့ဒ် တက္ကသိုလ် သည် ၊ အွန်လိုင်း မှ လှမ်းယူသုံးစွဲ ရန် ခွင့်ပြုထားသော စီမံချက် တစ်ခု ၏ အစိတ်အပိုင်း အဖြစ် စုဆောင်းထား ပြီး ၊ ကမ္ဘာ့ အကြီးဆုံး ဟု ပြောလို့ရသည် ။\n",
      "TARGET: The University of Oxford is currently digitising the songbooks and anthologies of the Harding Collection, said to be the world's largest such collection, as part of a project to allow online access.\n",
      "PREDICTED: The is a of the of the , , , , and .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ချီကာဂို မှ အသက်၂၆နှစ်အရွယ်ရှိ ၊ အေပါဗီတှာ ဒူဂန် သည် ၊ ၅၉ လမ်းမကြီး တွင် တောင်ဘက်သို့ သွားနေရာ မှ ၈၈ အဝေးပြေးလမ်း သို့ လမ်းဖြတ်ခုံး တွင် ဘယ်ဘက် လှည့် ရန် ကြိုးစားခဲ့သည် ဟု နပါဗီလီ ရဲများ က သတင်းပို့ခဲ့ပါသည် ။\n",
      "TARGET: Naperville police reported that Pavitra Durgam, a 26-year-old from Chicago, was traveling southbound on Route 59 and tried to turn left onto the ramp to Interstate 88.\n",
      "PREDICTED: The , , , was by , said that the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ဖေဖော်ဝါရီလ မှာ စုံတွဲ သုံးတွဲ ဟာ ပထမဆုံး လူမည်း ဆင့်နီလားစ်မြို့ ၏ မြို့တော်ကောင်စီ ရှေ့ မှာ လက်ထပ်ကတိစကား ပြောဖို့ ငြင်းဆိုခဲ့ တဲ့ အကြောင်း ကတော့ သူ ၏ အသားအရောင် ကြောင့် ဖြစ်တယ် ။\n",
      "TARGET: In February, three couples refused to pledge their wedding vows in front of the first black alderman of the city of Sint-Niklaas, because of the colour of his skin.\n",
      "PREDICTED: The of the of the of the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ဥရောပ သမဂ္ဂ ၏ အစီရင်ခံစာ တွင် အစ္စရေး အစိုးရ နှင့် ဂျေရုစလင် စီမံခန့်ခွဲရေး အဖွဲ့ တို့ ကို အာရပ် နေထိုင်သူများ နှိုင်းယှဉ် ကာ ဆောက်လုပ်ခွင့် ခွင့်ပြုမိန့်များ ၊ ကျန်းမာရေး စောက်ရှောက်မှုများ ၊ ပညာရေး ၊ မိလ္လာစနစ် နှင့် အခြား အရာများ နှင့် ပတ်သတ် ၍ ကွဲပြား ခြားနားနေမှု ကို ပြောဆိုထားသည် ။\n",
      "TARGET: The EU report claims the Israeli government and Jerusalem municipality discriminate against Arab residents with regard to building permits, health services, education, sanitation and more.\n",
      "PREDICTED: The of the of the of the , , , , and .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: သို့သော်လည်း ၊ စားသုံးသူ ထုတ်ကုန်များ သယ်ဆောင်လာသော ကုန်စည် ရထား သည် ၊ အနောက်သြစတေးလျ ၏ တောင်ဘက် တွင် လမ်း တို နိုင် သော်လည်း နောက်ထပ် ရက် အနည်းငယ်ကြာနိုင်သည် ။\n",
      "TARGET: However, the freight train was carrying consumer products, which may run short in the south of Western Australia in the next few days.\n",
      "PREDICTED: However , the of the of the , is a of the of the of the .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  3: 100%|████████████████████████████████████████████████████████████████████████████████████| 4521/4521 [20:39<00:00,  3.65it/s, loss=5.504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: လွန်ခဲ့တဲ့ အပတ် က ၊ ရက်ပါ ကန်နီ ဝက်စ် က အန်န်ဘီစီ လှူဒါန်း ပွဲ ဖြစ်သည့် ဟာရီကိန်း လေမုန်တိုင်း ချမ်းသာ ဘေးကင်းခွင့် အတွက် ဖျော်ဖြေပွဲ တစ် ခု အပေါ် ကို သဘောထား မှတ်ချက် တစ် ခု ချခဲ့သည် ၊ ထို မှတ်ချက် ထဲတွင် ဝက်စ် က “ ဂျော့ ဘုရှ် က လူ မည်း တွေ အကြောင်း ကို ဂရုမစိုက်ပါဘူး ။ ” ၊ ဟု အခိုင်အမာ ပြောဆိုခဲ့ ပြီး အငြင်းပွားမှု ကို ဖြစ်ပွားစေခဲ့သည် ။\n",
      "TARGET: Last week, rapper Kayne West made a remark on an NBC charity show A Concert for Hurricane Relief, in which West claimed that \"George Bush doesn't care about black people.\", which caused controversy.\n",
      "PREDICTED: In a statement , the of the , , , , , , .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ပထမဦးဆုံး ပတ်လည် မဲ ပေး ပြီးနောက် လိမ္မော်ရောင် ဒီမိုကရက်တစ် လှုပ်ရှားမှု အဖွဲ့ အနေဖြင့် နောက်ထပ် မဲ လိုအပ်ချက် အရ ဦးဆောင်မှု တစ် ခု စေ့စပ်စွာ ဆောင်ရွက်လာခဲ့သည် ။\n",
      "TARGET: After a first round of voting, the Orange Democratic Movement candidate had a narrow lead, necessitating another ballot.\n",
      "PREDICTED: The was by the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: နောက်ဆုံး ရှုံးထွက် ကန် ရန် သူတို့ ကို အခွင့်ရေးပေး ဖို့ ခုနစ်သင်းလုံး ကို နိုင်ထားသော အသင်းနှစ်သင်းလုံး သည် တနင်္ဂနွေနေ့ တွင် ပြိုင်ပွဲ ဝင်ခဲ့သည် ။\n",
      "TARGET: Both teams entered Sunday's match with a seven-game unbeaten streak to give them the chance to take the final playoff spot.\n",
      "PREDICTED: The .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: အဇာဘိုင်ဂျန် မြို့တော် ၊ ဘာကူ တွင် ဆန့်ကျင်ကန့်ကွက်သူ ထောင်ပေါင်းများစွာ က ယနေ့ ဆန္ဒထုတ်ဖော်ခဲ့သည် ။\n",
      "TARGET: Thousands of protestors demonstrated today in Baku, the capital of Azerbaijan.\n",
      "PREDICTED: , , and .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ဇွန်လ ၈ ရက်နေ့ က ပြုလုပ်သော တွေ့ဆုံမေးမြန်းခန်း တစ်ခု တွင် ၊ ဝန်ကြီးချုပ် ဂျွန် ဟိုဝဒ် က ၊ “ လိင်တူဆက်ဆံတာ ကို ဆန့်ကျင်ပြီး ခွဲခြားသတ်မှတ်တဲ့ မည်သည့် နည်းလမ်း မှ ငါ ရှာမတွေ့ဘူး “ လို့ ပြောခဲ့သည် ။\n",
      "TARGET: In an interview on June 8, Prime Minister John Howard said, \"I don’t see it in any way as discriminating against homosexuals.\"\n",
      "PREDICTED: The , , said that the of the , \" was and \" and .\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  4: 100%|████████████████████████████████████████████████████████████████████████████████████| 4521/4521 [20:18<00:00,  3.71it/s, loss=4.950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \" ဒါဟာ စက်ပိုင်းဆိုင်ရာ ပြသနာ တစ်ခု နှင့် ချိတ်ဆက်နေလား ၊ ပင်ပန်းနွမ်းနယ်မှု သို့မဟုတ် လုံးဝ ကွဲပြားခြားနားသော အခြေအနေ ကြောင့်လား ? \" ဟု ဘက်စ်ဆာရူး က အခင်းဖြစ်ပွားရာနေရာတွင် ပြောခဲ့သည် ။\n",
      "TARGET: \"Is this linked to a mechanical problem, drowsiness or a totally different cause?\" Bussereau said at the scene.\n",
      "PREDICTED: \" We have been to the of the , and I ' m not going to be ,\" said .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ထုတ်ပြန်ချက် တစ်ခု အတွက် ဝီကီသတင်းဋ္ဌာန သည် အေစီအမ်အေ ကို ဆက်သွယ်သည် ၊ သို့သော် ထို အစီရင်ခံစာ အရ ၊ အဲဒီမှာ တုန့်ပြန်ချက် လုံးဝမရှိပါ ။\n",
      "TARGET: Wikinews contacted the ACMA for a statement, but as of this report, there was no response.\n",
      "PREDICTED: The is the first of the , which is the , is the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ၁၇၀၈ ခုနှစ် ( မင်တန် ၏ သေဆုံးမှု ပြီးနောက် နှစ်ပေါင်း ၃၀ ကျော် ) က ၊ အောက်စဖို့ဒ် နှင့် ကင်းဘရစ် မစ်ဆဲလန် စာအုပ် ရှိ မေးခွန်း ကို ၊ အခြား အောက်စဖို့ဒ် ဝါရင့်ဆရာ ၊ ဒေါက်တာ အဘိဂဲလ် ဝိလျံ ၊ မှ \" မြို့ နှင့် ပတ်သက်သော အသိဉာဏ်ပြည့်စုံသော လူငယ်များ အတွက် မြို့ နှင့် ပတ်သက်သော အသိဉာဏ်ပြည့်စုံသော လူငယ်များ မှ ရေးသားခဲ့သည့် ကဗျာများ အစုလိုက် \" ကဲ့သို့ ဖော်ပြခဲ့သည် ။\n",
      "TARGET: The book in question, the Oxford and Cambridge Miscellany from 1708 (over 30 years after Milton's death), was described by another Oxford academic, Dr Abigail Williams, as \"a set of poems written by witty young men about town for witty young men about town\".\n",
      "PREDICTED: The of the of the of the , , , , , , , , said : \" The of the of the of the , , and , and , and , and\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \" လွန်ခဲ့သော ဆယ့်နှစ် လ ကျော် က တိုးတက်မှု အားလုံး သည် လက်ရှိ အခြေအနေ ၌ စားသုံးသူများ ၏ အကဲဖြတ်မှု နှင့် ၊ အစောပိုင်း အဆင့်များ တွင် နိမ့်ကျမှုများ မျှော်လင့်စရာ ရှိ သော်လည်း ၊ စားသုံးသူများ က စီးပွားရေး သည် ၂၀၀၆ ခုနှစ် တွင် ချဲ့ထွင် ရန် ဆက်လက်လုပ်ဆောင်လိမ့်မည် ဟု ယုံကြည်ကြသည် ။ \"\n",
      "TARGET: \"Even though all of the improvement over the past twelve months has been in consumers' assessment of current conditions, and expectations remain below earlier levels, consumers are confident that the economy will continue to expand in 2006.\"\n",
      "PREDICTED: \" We have a of in the of the of the of the of the of the of the of the of the , and , and .\"\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ရုတ်တရက်တိုက်ခိုက်ခြင်း တွင် ၊ ပိုင်ရိတ်ဘေး သို့ အဆက်အသွယ်မရှိသည့် များစွာသော ကိုယ်ပိုင် ဆာဗာများ ကိုလည်း ရဲ က ယူဆောင်ခဲ့သည် ။\n",
      "TARGET: In the raid, the police also took several private servers unconnected to The Pirate Bay.\n",
      "PREDICTED: The , which was by the , was by the .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  5: 100%|████████████████████████████████████████████████████████████████████████████████████| 4521/4521 [19:10<00:00,  3.93it/s, loss=5.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: အီးတီအေ က နွေရာသီ လများ အတွင်း စပိန် မြောက်ပိုင်း ရှိ ခရီးသွားဧည့် နေရာများ ကို ပုံမှန် ပစ်မှတ်ထားလေ့ ရှိသည် ။\n",
      "TARGET: ETA has routinely targeted tourist destinations in northern Spain during summer months.\n",
      "PREDICTED: The is expected to be to be in the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ဒီ ဖိနပ် တွေ ကို ၁၉၃၈ ခုနှစ် တွင် ရှာဖွေတွေ့ရှိခဲ့ ၊ ပြီး ယခု မတိုင်ခင် နှစ် ၁၀,၀၀၀ ခန့် အထိ ရက်စွဲတပ်ခဲ့သည် ။\n",
      "TARGET: These shoes were discovered in 1938, and have been dated to about 10,000 years before present.\n",
      "PREDICTED: The was in , , .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: အယ်ဂျာဇီရာ အစိုးရ သတင်း ဌာန အတွက် သတင်းပေးပို့သူ တစ်ဦး က \" မီးရှုးမီးပန်းဗျောက်အိုးများ ကြောင့် ဖြစ်ပွားခဲ့ရသည့် အဆိုပါ မတော်တဆမှု သည် အရေးကြီးလှသော လူ့အသက် ကို ဆုံးရှုံးမှု တစ်ခု ဖြစ်စေသော ပေါက်ကွဲမှု တစ်ခု ဖြစ်ခဲ့တယ် ဆိုတာကို ပြောခြင်း ဖြင့် တာဝန်ရှိသူများ ကို ဆင့်ခေါ်ပြောဆိုမှုရှိတယ် \" ဆိုတာကို မှတ်ချက်ချခဲ့သည် ။\n",
      "TARGET: A reporter for the Al Jazeera news agency remarked: \"Officials have been cited as saying that the incident may have been caused by pyrotechnics that caused an explosion leading to a significant loss of life.\"\n",
      "PREDICTED: The .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ဂျီအန်ယူ ဟာဒ် စီမံကိန်း အဆိုအရ ၊ “ အနာဂတ် တွင် အဆိုပါ ဟာဒ် သည် ၊ ဖြစ်ကောင်းဖြစ်နိုင်လိမ့်မည် နှင့် ၊ အခြား ဟာဒ်ဝဲ တည်ဆောက်ပုံ သို့မဟုတ် အခြား မိုက်ခရိုကာနယ်များ တွင် တပ်ဆင်သင့်သည် ။ ”\n",
      "TARGET: According to the GNU Hurd project, \"The Hurd should, and probably will, be ported to other hardware architectures or other microkernels in the future\" .\n",
      "PREDICTED: \" The of the of the , , and , , , .\"\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ဩစတြေးလျနိုင်ငံ ၊ မဲလ်ဘုန်း ရှိ ရဲတပ်ဖွဲ. က ယနေ. မနက် မြို.တော် ၏ စီးပွားရေး အချက်အခြာ ခရိုင် တွင် လူတစ်ယောက် ကို သတ်ခဲ့ ပြီး အခြားသူ နှစ်ယောက် ကို ထိခိုက်ဒဏ်ရာရစေခဲ့ သော သေနတ်သမား တစ်ယောက် အတွက် ရှာဖွေခြင်း ပြုလုပ်ကြသည် ။\n",
      "TARGET: Police in Melbourne, Australia are on the hunt for a gunman who killed a man and injured two others in the city's central business district this morning.\n",
      "PREDICTED: In the last year , the .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  6: 100%|████████████████████████████████████████████████████████████████████████████████████| 4521/4521 [18:49<00:00,  4.00it/s, loss=5.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ရစ် ဖလဲ ( အမည် ရင်း ရစ်ချက် ဖလဲဟာ ) အဖမ်းခံရန် အတွက် ဝရမ်း ထုထ်ခံထားရသည် ။\n",
      "TARGET: Ric Flair (real name Richard Fliehr) has a warrant out for his arrest.\n",
      "PREDICTED: The ( ) is not the first time for the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ဘာကူမြို့ ဆင်ခြေဖုံး သို့ လမ်းခုလတ် တွင် လူ များစွာ တို့ ရဲတပ်ဖွဲ့ ၏ ဟန့်တားခြင်းခံခဲ့ရသည် ။\n",
      "TARGET: Many people were stopped by police on their way to the outskirts of Baku.\n",
      "PREDICTED: The police said that they were not the of the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ရီယို တင်တို သံ သတ္ထုရိုင်း မီးရထား တစ်စီး သည် ပီဘာရာ ၌ ၊ တွမ်ပရိုက်စ် မှ ဒမ်ပိုင်ယာလမ်း သို့ လမ်းခွဲ တစ်ခု ပေါ်တွင် ကြာသပတေးနေ့ ၂၉ က လမ်းချော်ခဲ့သည် ။\n",
      "TARGET: A Rio Tinto iron ore train derailled at Pilbara, on a spur of the Tom Price to Dampier line on Thursday 29.\n",
      "PREDICTED: The .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: သောကြာ နေ့ အစောပိုင်း တွင် ၊ အမေရိကန် ဥပဒေပြုအမတ် များ က သန်းခေါင်ယံ ထိ အလုပ် တစ်ခု ကို စမ်းသပ်ရှာဖွေခဲ့ကြ ၊ ပေမယ့် ရီပတ်ဘလီကန် ပါတီ သတင်းရပ်ကွက် က အလုပ် တစ်ခု ဖြစ်မြောက် ရန် ရှိသည် ကို နောက်ပိုင်း အစီရင်ခံခဲ့ကြသည် ဟု တောင်းဆိုချက် များ ကို ပယ်ချခဲ့သည် ။\n",
      "TARGET: Earlier on Friday, US legislators deflected claims that a midnight deal was being worked out, but Republican Party sources later reported that a deal was likely to be reached.\n",
      "PREDICTED: The , a , said that the was not in the , and that the was not the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ပိတ်ထားခြင်းခံရသော ဝီကီပီးဒီယား ၏ အစိတ်အပိုင်းများ တွင် ဝီကီနယူး ပံပိုးပေးသူ ဒေးဗစ် ရှန့်ဘုန်း ရိုက်ထားသော ဒေးဗစ် ရှန့်ဘုန်း ၏ အရွယ်ရောက်သူ ရုပ်ရှင် ပြုလုပ်ခြင်း ဟုခေါင်းစဉ်တပ်ထားသော ဓာတ်ပုံ တစ်ပုံ ပါဝင်သည် ။\n",
      "TARGET: Portions of Wikipedia blocked include a photo taken by Wikinews contributor David Shankbone titled The making of an adult film by David Shankbone.\n",
      "PREDICTED: The of the of the .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  7: 100%|████████████████████████████████████████████████████████████████████████████████████| 4521/4521 [18:45<00:00,  4.02it/s, loss=5.052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: အီရတ် ၌ နှစ်ရက် တွင် ဗုံး လေးလုံး ကြဲခြင်း က ထိခိုက်ဒဏ်ရာ အမြောက်အများ နှင့်အတူ ၊ အနည်းဆုံး သေဆုံးသူ ၁၃၀ အထက် ရှိခဲ့သည် ။\n",
      "TARGET: Four bombings in two days have left over 130 dead in Iraq, with at least as many injured.\n",
      "PREDICTED: The , which was by the , was by the police in the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: “ ကျွန်တော် သည် ကျွန်တော့် ဘ၀ ရဲ့ ဒီလို အခြေအနေမျိုး မှာ ကျွန်တော် ကိုကင်း မသုံးစွဲပါ ၊ ကျွန်တော့် ကိုယ်ကိုယ်ကိုယ် သတ်သေမကျူးလွန် ဖို့ ဆန္ဒ မရှိသမျှ ကာလပတ်လုံး ပေါ့နော် ” ။\n",
      "TARGET: \"I wouldn't take cocaine at this point in my life unless I wished to commit suicide.\"\n",
      "PREDICTED: \" We ' re going to get to the next day , but we ' re going to get to the of the people ,\" he said .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ၂၀၀၆ ခုနှစ် ၊ ဒီဇင်ဘာ လ ၂၂ ရက် နေ့ တွင် အမေရိကန်ပြည်ထောင်စု ရှိ အိတ်စ်ဘောက်စ် ၃၆၀ ဗီဒီယို ဂိမ်း ခလုတ်ခုံ အားလုံး အတွက် ၎င်း ၏ အာမခံသက်တမ်း ကို ၄င်း က တစ် နှစ် တိုးခဲ့ပါ ကြောင်း မိုက်ခရိုဆော့ က ကြေညာခဲ့သည် ။\n",
      "TARGET: On December 22, 2006 Microsoft has announced that it has extended its warranty for all Xbox 360 video game consoles to one year in the United States.\n",
      "PREDICTED: On November 7 , 2008 , the United States government has been killed in the United States of Representatives , and it was the first time of the of the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ခြောက်ယောက် ချက်ချင်း သေဆုံးခဲ့ ပြီး ၊ ခုနှစ်ယောက်မြောက် ဒုက္ခသည် သည် ဖြစ်ပြီး မကြာမီ သေဆုံးသွားခဲ့သည် ။\n",
      "TARGET: Six died immediately, and the seventh victim died shortly afterwards.\n",
      "PREDICTED: The plane was killed , and was injured and injured and injured .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ဝီကီလိသတင်းဋ္ဌာန အားဖြင့် ပေါက်ကြားသွားသော အားလုံးပေါင်း စာရင်း သုံးခု သည် ဝီကီပီးဒီးယား အင်တာနက်လိပ်စာများ တွင် ပေါ်ပေါက်လာသည် ။\n",
      "TARGET: The Wikipedia URLs appear on all three lists leaked by Wikileaks.\n",
      "PREDICTED: The .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  8: 100%|████████████████████████████████████████████████████████████████████████████████████| 4521/4521 [18:43<00:00,  4.02it/s, loss=4.714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ၎င်းကို အမေရိကန် ၊ ဘရာဗို နှင့် ဂျီဖိုး အစရှိသော များပြားလှသည့် ကေဘယ်လ် ကွန်ယက် များ နှင့် အင်တာနက် ပေါ် များ တွင် လည်း ပြသခဲ့ကြသည် ။\n",
      "TARGET: It was also shown on the Internet and many cable networks such as USA, Bravo and G4.\n",
      "PREDICTED: The , however , was by the and and .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ထို့အပြင် အမတ်နေရာများ ဆုံးရှုံးမှုဖြစ်ခြင်း သည် အယ်လ်ဒီပီ ၏ ညွန့်ပေါင်း တွဲဘက် နယူး ကိုမီတို ( အင်န်ကေပီ ) ပါတီ လည်း ဖြစ်ခဲ့ပြီး ၎င်းပါတီ သည် အတိုင်ပင်ခံ အမတ်နေရာ ၄ နေရာ ဆုံးရှုံးခဲ့ရ ပြီး အခု နေရာ ၂၀ ကို ထိန်းသိမ်းထားသည် ။\n",
      "TARGET: Also losing seats was LDP's coalition partner, New Komeito Party (NKP), which lost 4 councillor seats and now holds 20.\n",
      "PREDICTED: The of the ( ) have been and has been by the ( ) and its .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: သင်္ဘော ၃၅၀၄ မှ အမှုထမ်း သင်္ဘော သား များ ကို ဖိလစ်ပိုင် လေ တပ် မှ စိုက်ဘေ ကျွန်း တွင် နေရာ ချ ပေး ထား ခဲ့ပါတယ် ။\n",
      "TARGET: Vessel 3504's crew were located on Sibay Island by the Philippine Air Force.\n",
      "PREDICTED: The of the have been in the of .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ရစ် ဖလဲ ( အမည် ရင်း ရစ်ချက် ဖလဲဟာ ) အဖမ်းခံရန် အတွက် ဝရမ်း ထုထ်ခံထားရသည် ။\n",
      "TARGET: Ric Flair (real name Richard Fliehr) has a warrant out for his arrest.\n",
      "PREDICTED: The of the ( ) is not yet to be the first time .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ခွဲခြားထားသော စာချုပ်မူကြမ်း တွင် ဘုရှ် အုပ်ချုပ်ရေးအဖွဲ့ က သိလျက် လက်ခံထားသော အဆင့်မြှင့်ထားသော စုံစမ်းထောက်လှမ်းမှု နည်းပညာ ၀ါတာဘရော့ဒ် နှင့် ဝေဖန်သူ များ ထိတ်လန့်စရာ မသင်္ကာဖွယ်ရာ များ ဆန့်ကျင် အသုံးချခြင်း အတွက် ၊ နှိပ်စက်ညှင်းပန်းခြင်း ကို သတ်မှတ် သည့် ၊ အခြား နည်းလမ်း များ အပါအဝင် လက်ခံထားသည် ။\n",
      "TARGET: Classified memos have revealed that the Bush administration knowingly endorsed enhanced interrogation techniques including waterboarding and other methods, that critics call torture, for use against terror suspects.\n",
      "PREDICTED: The of the , which is in the , has been by the , and , has been by .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  9: 100%|████████████████████████████████████████████████████████████████████████████████████| 4521/4521 [18:44<00:00,  4.02it/s, loss=4.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: စုံစမ်းစစ်ဆေးမှုများ သည် ဖလင်းဒါး လမ်းသွယ် နှင့် ဝီလီယမ် လမ်းဆုံ မှာ ဆောင်ရွက်ခဲ့ သောကြောင့် ၊ အရာရှိများ က ရုံး အလုပ်သမားများ အားလုံး ကို အဆောက်အအုံတွင်း နေရစ် ရန် သတိပေးခဲ့ ပြီး ၊ ဖလင်းဒါးစ် လမ်း ဘူတာရုံ က မီးရထား လေ့ကျင့်ပေးခြင်းများ သည် ယာယီ ရပ်တန့်ထားဆဲ ဖြစ်သည် ။\n",
      "TARGET: Officials warned all office workers to remain indoors, and train services to Flinders Street Station were temporarily on hold, as investigations were carried out on the intersection of Flinders Lane and William Street.\n",
      "PREDICTED: The investigation was also by the , and the , and the crew had been and by the and .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: နောက်ပိုင်း တွင် သူ သည် ဟာဗာနား သို့ ရွှေ့ပြောင်းခဲ့ ပြီး ထို နေရာ တွင် သူ သည် သူ ၏ ဆေးဘက်ဆိုင်ရာ လုပ်ငန်းများ ကို ဆက်လုပ်ခဲ့ပါသည် ။\n",
      "TARGET: Later he moved to Havana where he continued his medical work.\n",
      "PREDICTED: .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: တူညီသော စနစ် သည် အပြည်ပြည်ဆိုင်ရာ အဆင့်အတန်း တွင် ကျင့်သုံးသည် ။\n",
      "TARGET: The same standard can be enforced on an international level.\n",
      "PREDICTED: He is also expected to be the first time to be .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ၄င်း က ပေါက်ကွဲ တော့မည့် ဗုံး တစ် လုံး နဲ့တူ သည် ၊ ၄င်း က တတိယ ကမ္ဘာ စစ် နဲ့ တူ သည် လို့ ၊ ရပ်ကွက် ထဲတွင် ပန်းချီ အနုပညာ စတူဒီယို ပိုင်ဆိုင် သော မာ့စု(ဒ်) အိုလူဖနီ က ၊ ပြော ခဲ့သည် ။\n",
      "TARGET: It looks like a bomb went off, it looks like World War III, said Mahsud Olufani, who has an art studio in the neighbourhood.\n",
      "PREDICTED: It was also the first time , but the first time , was the first time , and the first time , the first time they had been in the past .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: မီးတောက် ထဲတွင် ကွဲအက် ပြီး ပျော်\n",
      "TARGET:  Cracks and rejoices in the Flame\n",
      "PREDICTED: The is the of the .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  10: 100%|███████████████████████████████████████████████████████████████████████████████████| 4521/4521 [18:48<00:00,  4.01it/s, loss=4.035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ခရစ်ယာန်ဂိုဏ်းအုပ်များအဖွဲ့အစည်းက ပြုလုပ်သော ကြေငြာချက် အရ အင်္ဂလန်၏ အသင်းတော်သည် တရားဝင်လက်တွဲဖော်ရှိသူ လက်မထပ်ရသေးသော လိင်တူချစ်သူအမျိုးသားများကို ဂိုဏ်းအုပ်ဆရာတော်များ ဖြစ်ခွင့်ပေးရန် စီစဉ်နေသည် ။\n",
      "TARGET: The Church of England are planning to allow gay men who are celibate and in a civil partnership to become bishops according to an announcement made by the House of Bishops.\n",
      "PREDICTED: The UN ' s is a that the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ကနေဒီယန် သမိုင်း တွင် အကြွေးငွေ ပေးချေမှု ကြေညာမှု့ သည် အကြီးဆုံး ဖြစ်ခဲ့သည် ။\n",
      "TARGET: The announced debt payment is the largest in Canadian history.\n",
      "PREDICTED: The of the was the first time for the country .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ကလင်တန် ၏ အစီအစဥ် ကြောင့် ဆန္ဒပြပွဲ သည် တစ် ပတ် နှောင့်နှေးခဲ့ရသည် ။\n",
      "TARGET: The rally was delayed one week due to Clinton's schedule.\n",
      "PREDICTED: The of the was closed and the of the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ဝီကီသတင်းဋ္ဌာနသည် လည်းဘဲ ဖိုင် ကို ဖျက်ခဲ့သည့် ဟု အတည်ပြုထားသော မစ္စတာ ဝေး ကို ဆက်သွယ်ခဲ့သည် အဘယ်ကြောင့်ဆိုသော် သူသည် ဤကဲ့သို့ ပြုလုပ် ရန် ပြောခြင်းခံရ၍ဖြစ်သည် ။\n",
      "TARGET: Wikinews also contacted Mr. Wales who confirmed the file was deleted because he was asked to do so.\n",
      "PREDICTED: Wikinews has stated that he was a of and that he was a to the world ' s .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: “ အဆိုပါ တိုက်ခိုက်မှု သည် ကြောက်တတ် ပြီး သစ္စာဖောက်ခြင်း သာမက ၊ ဂျာမဏီ နိုင်ငံ ၏ နာမည် ကို လည်းပဲ ပျက်ဆီးစေသည် ” ၊ ဟု ဇဂီဘီအယ် ကို ပြောကြားခဲ့သည် ၊ ဒုက္ခသည် များ နှင့် သူတို့ ၏ ဆွေမျိုး များ က သူ သည် “ လျင်မြန်စွာ ဆုံးဖြတ်ချက်ချ ရန် ပြင်ဆင်သူ များ ကို ခေါ်ဆောင်လာခြင်း ဖြင့် ထိုအရာ ကို မျှော်လင်ခဲ့သည် ၊ သင် သည် ကြိုးစားမှု အချို့ ကို ……. ရှာဖွေနိုင် ပြီး ဂျာမဏီ ဆီသို့ မည်သည့် ရန်ငြိုး ကို မဆို လက်ခံမပေးပါ ဟု ” ပြောကြားခဲ့သည် ။\n",
      "TARGET: \"The attack was not only cowardly and perfidious, but also damaged Germany's reputation,\" said Sagebiel, telling victims and their relatives he \"hoped that by our bringing the perpetrator to justice swiftly, you can find some comfort... and will not harbour any rancour towards Germany.\"\n",
      "PREDICTED: \" The fact that the of the , the company , the of the , have been by the of the , , the of the of the , the of the of the , the of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  11: 100%|███████████████████████████████████████████████████████████████████████████████████| 4521/4521 [18:40<00:00,  4.04it/s, loss=4.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: လျှပ်စစ် လိုင်း များ ၊ ကြော်ငြာ ဆိုင်းဘုတ် များ ၊ နှင့် ရုံး ကုလားထိုင် များ တောင် ပါဝင် သော ၊ အပျက်အစီး များ ကြောင့် ရဲ တပ်ဖွဲ့ က စီအန်အန် ဌာန အနီး က လမ်း အများအပြား ကို ပိတ် ခဲ့ သည် ။\n",
      "TARGET: Police closed several streets near the CNN Center because of the debris, which included power lines, billboards, and even office chairs.\n",
      "PREDICTED: , , and , were by the and .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ဩစတြေးလျရဲ့ ’ ဆင်ဆာလုပ်ငန်း လွှတ်တော်အမတ် ’ စတီဗန်ကွန်လွေး မှာ သူ၏ နည်းလမ်း ရှိတယ် ဆိုရင် ၊ ဩစတြေး သည် မဖြစ်မနေ အင်တာနက် ဆင်ဆာလုပ်ငန်း အုပ်ချုပ်မှုစနစ် တစ်ခု ရှိသည့် ပထမဆုံး အနောက်နိုင်ငံ ဖြစ်လာလိမ့်မည် ဟု ဝီကီလိခ် က သူတို့ရဲ့ ဝက်ဘ်ဆိုက် ပေါ်တွင် နောက်ထပ် ကြေငြာချက် မှာ ပြောခဲ့ပါ တယ် ။\n",
      "TARGET: \"If Australia's 'Senator for Censorship', Steven Conroy, has his way, Australia will be the first Western country to have a mandatory Internet censorship regime,\" said Wikileaks in another statement on their website.\n",
      "PREDICTED: The Australian House of Representatives ' s leader , , said that the two men ' s were in a statement that the two - day trial will be by the U . S . military . and US forces .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: လျှို့ဝှက် မဲ ရှိသည့် နေရာ ကို ခင်ဗျား ကျွန်တော်တို့ ကို ပြနိုင်သလား ?\n",
      "TARGET: Can you show us where it says secret ballot?\n",
      "PREDICTED: The of the and are not known to be .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ကောင်စီ အဖွဲ့ဝင်များ သည် မိမိတို့ သက်ဆိုင်ရာ မြို့တော် အတွက် တိုင်ပင် ဆွေးနွေးချိန် က “ အချို့ အကြောင်း အရာ အများစု ကို ကျွန်တော်တို့ ( သောကြာနေ့ ) တွင် အတည်ပြုလိမ့်မည် ” ဟု “ နောက်ဆုံး ရှင်းလင်းချက် ကို ” သူ က ဆက်ပြောသည် ။\n",
      "TARGET: \"Most likely we will have something adopted (Friday)\" once council members consult their respective capitals for \"final clearance,\" he added.\n",
      "PREDICTED: The ( ) \" \" and \" \" ( ) to the \".\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: မင်တန် သည် ကာရန်ညီခြင်း ကို သုံးခြင်း ထက် ကာရန်မညီသော ကဗျာ ကို ရေးသားခဲ့ သည့်အပြင် မတူညီတဲ့ ကဗျာလင်္ကာနှင့်ဆိုင်သော စည်းချက်များ ကို အသုံးပြုခဲ့သည် ၊ စစ်စစ်မှာတော့ လက်ဖြင့်ရေးသားထားသော ကဗျာ က ၎င်း သည် ပို၍ အတွေ့အကြုံရှိသော မိန်းမများ နှင့် ငယ်ရွယ်သော မိန်းမများ ၏ လိင်ပိုင်းဆိုင်ရာ အပြုအမူကို မီးပေါ်တွင် စိမ်းစိုသော သစ်သား နှင့် ခြောက်သွေ့သော သစ်သား တို့နှင့် သူတို့ ကို နှိုင်းယှဥ်ခြင်း ဖြင့် ကွဲပြားမှုကိုယှဥ်ပြထားသည့်အဆုံး ကာရန်တူလုံးရေညီ နှစ်ကြောင်းကဗျာ ၏ ရိုးရှင်းသော ပုံစံ တစ်ခု ကို အသုံးပြုထားသည် ။\n",
      "TARGET: Milton wrote in blank verse rather than using rhyme and used differing poetic rhythms, whereas the handwritten poem uses a simple pattern of rhyming couplets as it contrasts the sexual behaviour of young women and more experienced women by comparing them to green wood and dry wood upon a fire.\n",
      "PREDICTED: The United Nations has been by the United Nations and its European Union , and have suggested that the use of the United Nations and that the United States has not been their , and that they have not been to their , and that they have not\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  12: 100%|███████████████████████████████████████████████████████████████████████████████████| 4521/4521 [18:45<00:00,  4.02it/s, loss=4.121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: သမုဒ္ဒရာ-မျက်နှာပြင် အပူချိန် သည် လွန်ခဲ့သော ၁၂၈ နှစ် ကာလ အတွင်း ဒုတိယ အပူနွေးဆုံး ဖြစ်ခဲ့သည့် အချိန်တွင် ၊ တစ်ကမ္ဘာလုံးဆိုင်ရာ မြေ-မျက်နှာပြင် အပူချိန် သည် ပြင်းထန်သော အီးအယ်လ် နီနို ၁၉၉၇-၁၉၉၈ အပိုင်း အတောအတွင်း ရှိခဲ့သော မှတ်တမ်း ထက် ၀.၀၆ ဒီဂရီ စင်တီဂရိတ် ခန့် ထက် ပို၍ အေးခဲနေ ပြီး ၊ မှတ်တမ်း တွင် အပူနွေးဆုံး ဖြစ်ခဲ့သည် ။\n",
      "TARGET: The global land-surface temperature was the warmest on record, while the ocean-surface temperature tied for second warmest in the past 128 year period, approximately 0.06 degrees C cooler than the record established during the strong El Niño episode of 1997-1998.\n",
      "PREDICTED: The company was founded in - and was in - , which was - , in 1999 , the - - and - - in the - .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: လက်ရှိ စွဲချက်တင်ခြင်း က ၎င်း စွပ်စွဲချက်များ အပေါ် မှ မည်သည့် စွဲချက်များ မှ ပါဝင်ခြင်း မရှိပါ ။\n",
      "TARGET: The present indictment does not include any charges over these allegations.\n",
      "PREDICTED: The of the were not available for the incident .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ရဲ က မိုးကြီးမှု သည် ဧရိယာ ကို အကျိုးသက်ရောက်စေသည် ဟု ပြော ပြီး ၊ ပြည်နယ် သြစတေးလျ မီးရထားလမ်း ကော်ပိုရေးရှင်း က ပျက်စီးမှုများ ရှင်းလင်း ပြီး လမ်း ကို ပြန်ပြုပြင် ရန်အတွက် အသစ် အသုံးပြုရန် လမ်း တည်ဆောက် ရန် လိုအပ်သည် ဟု ပြောသည် ။\n",
      "TARGET: Police say heavy rain has affected the area, and the federal Australian Rail Track Corporation say that a new access road needs to be built in order to clear the wreckage and repair the track.\n",
      "PREDICTED: Police believe the police are looking to do not believe the threat , and that the police are not going to have enough the situation .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: သူတို့ သည် လော့စ် အင်ဂျလိ ဂလက်စီ ကို ၂၂-၅ ပစ်ထုတ်ခြင်း ဖြင့် ချီကာဂို ဖဲလ် သည် ကစားပွဲ ကို ထိန်းချုပ်ခဲ့သည် ။\n",
      "TARGET: Chicago Fire controlled the game as they outshot Los Angeles Galaxy 22-5.\n",
      "PREDICTED: They also took the lead to a - 25 match and scored a 25 - 25 match .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: သူတို့ ၏ သရုပ်ဖော်ပြပွဲ အတွင်း သူတို့ ထုတ်ဖယ်ထားသော အချက် သည် သရုပ်ဖော်ပွဲ သည် အသုံးပြု နိုင် ရန် ဆော့ဝဲလ် အတွက် ဖြစ်ခဲ့သည် ၊ အမှန်တကယ် ဟက်ဝဲလ် အတွက် မဟုတ်ပါ ။\n",
      "TARGET: During their demo they omitted the fact that the demo was for the software to be in use, not the actual hardware.\n",
      "PREDICTED: Their software is named as the , which they will be held in the .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  13: 100%|███████████████████████████████████████████████████████████████████████████████████| 4521/4521 [18:47<00:00,  4.01it/s, loss=4.536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: အန်အင်အက်ဆောရဲလာ ၏ မိန့်ခွန်း သည် သူပုန် အုပ်စု သည် ဆီးရီးယား ၏ စစ်ပွဲ တွင် တိုက်ရိုက် ပါဝင်ခဲ့ကြကြောင်း အတည်ပြုခဲ့ ပြီး ၊ သူ တို့ သည် လက်ဗနွန် ၏ နယ်စပ် ကို သူပုန်များ ၏ ထိန်းချုပ်မှု မှ ကာကွယ်သွားမည် ဟု ကတိပြုခဲ့သည် ။\n",
      "TARGET: Nasrallah's speech confirmed the group are directly involved in Syria's war, and pledged they would protect Lebanon's borders from the control of rebels.\n",
      "PREDICTED: The ' s defense minister , , said that the judge would not be allowed to his own and that his work .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ဝေဖန်သူများ က ကွန်ဆာဗေးတစ် အစိုးရ သည် သူတို့ သည် စတင် ရုံးထိုင် သောအခါ တွင် ကနေဒါ နိုင်ငံသားများ က အခွန်အခ လွန်စွာ ကောက်ခံခြင်းခံခဲ့ရသည် ကို ပြောကြားခဲ့သည် ဟု ထောက်ပြခဲ့ ပြီး ၊ \" အံ့ဩဖွယ် အပိုအလျှံ \" ပပျောက် ရန် ခိုင်ခိုင်မာမာကတိပေးခဲ့သည် ။\n",
      "TARGET: Critics point out that the Conservative government had said Canadians were overtaxed when they took office, and had vowed to eliminate \"surplus surprises\".\n",
      "PREDICTED: said , \" The of the people were killed and at least one person died in the time when they were killed ,\" and that the bomb was not taken to the scene .\"\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \" သုံး နာရီ ကြာ တော်လှန်ရေး \" တစ်ခု က တိုင်းပြည် မှာ ဘာကိုမျှ ပြောင်းလဲနိုင်မည်မဟုတ် ဟု အဇူရီလူမျိုး တစ်ချို့ က ယုံကြည်သည် ။\n",
      "TARGET: Some Azeris believe that a \"three hour revolution\" wouldn't change anything in the country.\n",
      "PREDICTED: \" The of the of the are not and we will not be able to make any way .\"\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \" ဥမင် ထဲ မှာ ဖိနပ် သို့မဟုတ် အခြား အရာဝတ္ထု များ က ဘာ လုပ်နေကြ သ လဲ သို့မဟုတ် ဥမင် ၏ ရည်ရွယ်ချက် က ဘာလဲ ဆို တာ ကျနော် တို့ ယခုတိုင် မသိရပါဘူး \" ၊ ဟု ပင်ဟာစီ က ပြောသည် ၊ \" ကျွန်တော် တို့ ဟာ ဥမင် ၏ နောက်ကျော မှာ ကလေး များ ရဲ့ သင်္ချိုင်းဂူ ရှိသည် ကို သိရ ပေမယ့် ဤ ကွဲပြားခြားနားသော အရာဝတ္ထု များ အားလုံး ကို အတူတကွ တွေ့ရှိခဲ့ သောကြောင့် ကျွန်တော် တို့ ဟာ မည်သည့် သေချာခြင်း ကို မပြောနိုင် သော ဤ ကာလ အကြောင်း ကို အနည်းငယ် သာ သိကြတယ် ။ \"\n",
      "TARGET: \"We do not know yet what the shoe or other objects were doing in the cave or what the purpose of the cave was,\" said Pinhasi, \"We know that there are children's graves at the back of the cave but so little is known about this period that we cannot say with any certainty why all these different objects were found together\".\n",
      "PREDICTED: \" The of the of the of the of the of the ,\" said , \" , , adding that , \" The of the of the of the of the of the world we are not that we will\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ခရစ် ဟူတွန် ကို နယူးကက်စ်တယ် ယူနိုင်တက် အင်္ဂလိပ် ဘောလုံး ကလပ် ၏ အမြဲတမ်း မန်နေဂျာ ခန့်ခဲ့သည် ၊ အရင် က ၎င်း ၏ ယာယီ အခွင့်အာဏာရှိသော မန်နေဂျာ အဖြစ် ရာသီ ၏ အစ ကတည်းက အမှုထမ်းခဲ့သည် ။\n",
      "TARGET: Chris Hughton has been appointed permanent manager of English football club Newcastle United, having previously served as its temporary caretaker manager since the start of the season.\n",
      "PREDICTED: The of the have been found in its , but it was not the first time of the of its in the of , England .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  14: 100%|███████████████████████████████████████████████████████████████████████████████████| 4521/4521 [18:39<00:00,  4.04it/s, loss=4.225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ယူကေ လေ့လာမှု ၏ အလုပ်လုပ်ပေးနေသော သီအိုရီ များ ကဲ့သို့ ၊ ကလေး များ ကို ထိတွေ့ ကူးစက်ခြင်း သည် ကလေးများအဖြစ်များသော ကင်ဆာ ရောဂါ များ ဖွံ့ဖြိုးမှု -- တစ်ခု နှင့် ဆက်နွယ်နေ သော သီအိုရီ များ သည် ၁၉၄၀ နီးပါး ကတည်းက ရှိခဲ့ ၊ ပြီး မစွဲကပ်စေသော စိန်ခေါ်မှု များ ကင်းမဲ့ခြင်း က ရောဂါကူးစက်ခြင်း ၏ အချို့သော အမျိုးအစား များ ၏ ရလဒ် တစ်ခု သည် နှောင့်နှေး သောကြောင့် လူကီးမီးယား ဖြစ်ထွန်းလာ သော ၊ အခြား အချက် တစ်ခု ဖြစ်လာသည် လို့ ပြောကြားခဲ့သည် ။\n",
      "TARGET: Theories have been around since the 1940s that childhood exposure to infection was related to the development of childhood leukaemia — one, like the working theory of the UK study, said that lack of immune challenge was a factor, another that leukaemia developed as a delayed result of some type of infection.\n",
      "PREDICTED: The Australian Capital Territory Territory has announced their plans to the of the of the , who had been a of since the Australian Capital Territory — a , in the Australian Capital Territory , and the Australian Capital Territory , have announced\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: သောကြာနေ့ ဆုတောင်းခြင်း မတိုင်မီ - လူပြည့်ကျပ်နေ လိမ့်မည့် အချိန် တစ်ခု ၌ ရှီအာ ဂူသင်္ချိုင်း ၏ ဂိတ် နှစ်ခု ကို ပစ်မှတ်ထားခြင်းခံခဲ့ရသည် ။\n",
      "TARGET: Two gates of the Shia tomb were targeted shortly before the Friday prayer - a time at which it would be crowded.\n",
      "PREDICTED: Friday morning the jury had been in a of the - at the in the - .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ထိုနေ့က ၊ ကုန်တွဲ ၄၀ နှင့် ကုန်စည် ရထား တစ်စင်း သည် ကယ်ဂူးလိုင်း ၏ အရှေ့ဘက် တွင် လမ်းချော်ခဲ့ပီး ၊ လူ သုံးဦး ဒဏ်ရာရခဲ့သည် ။\n",
      "TARGET: The following day, a freight train with 40 wagons derailed east of Kalgoorlie, injuring three people.\n",
      "PREDICTED: Then , it was his career and his father , had received a penalty .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: “ ပြုတ်ကျ သွားသော ပစ္စည်း များ နှင့် ပျက်စီးသွား သော ပြတင်းပေါက် များ အများကြီး ရှိ ခဲ့ တယ် ၊ ” လို့ ဟိုတယ် မှ ကားပါကင် ထိုးပေးသူ တာရန့်(စ်) အီဗန်(စ်) က ၊ ပြော ခဲ့ သည် ။\n",
      "TARGET: \" There was a lot of windows breaking and stuff falling,\" said Terrence Evans, a valet at the hotel.\n",
      "PREDICTED: \" The Australian government has announced that the Australian government ( ) has been ,\" said , a senior foreign minister .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: အနည်းဆုံး စစ်သား ၂ ေယာက် လေယာဉ် ပေါ်တွင် ရှိကြ သော်လည်း ၎င်းတို့ ၏ အခြေအနေ ကို မသိရပါ ။\n",
      "TARGET: At least 2 soldiers were on board the aircraft, but their condition is not known.\n",
      "PREDICTED: The two - five - year - old has been found guilty of the crash .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  15: 100%|███████████████████████████████████████████████████████████████████████████████████| 4521/4521 [18:47<00:00,  4.01it/s, loss=3.526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: စပင်ဆာ ဒရိုင်ဒန် သည် အဆိုပါ အဖွဲ့ ၏ အအောင်မြင်ဆုံးအချိန် မှာ ဂျက်ဖာဆန် အဲပလိန်း ၏ အဖွဲ့ဝင် တစ်ယောက် ဖြစ်ခဲ့သည် ။\n",
      "TARGET: Spencer Dryden was a member of Jefferson Airplane during the band's heyday.\n",
      "PREDICTED: The first leg of the match was the first half of the first half of the match .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: အယ်အက်စ်ဘီ ၃.၁ သည် လင်းနစ် ၏ အဓိကကျသော အပိုင်းများ ကို ပိုမိုလွယ်ကူသော တီထွင်မှု ပြုလုပ် ရန် စံသတ်မှတ်ထားသည် ။\n",
      "TARGET: LSB 3.1 standardizes core pieces of Linux to make development easier.\n",
      "PREDICTED: The of the is the largest in the world for the weekend .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ပြောရေးဆိုခွင့်ရှိသူ က ဆွေးနွေးမှု တစ် ခု ကို ဘယ်လို စတင်သင့် ကြောင်း ဆုံးဖြတ်တဲ့ အချိန် မှာ ဥပဒေရေးဆွဲသူ ရိုင်လာအိုဒင်နာဂါ ၏ ဆန့်ကျင်ဘက် လိမ္မော်ရောင် ဒီမိုကရက်တစ် လှုပ်ရှားမှု အဖွဲ့ ကို မစ္စတာ ကီဘာကီ ၏ ထောက်ခံသူများ က လျှို့ဝှက် မဲ အဖြစ် တောင်းဆို ခဲ့ရာ ပြောရေးဆိုခွင့်ရှိသူများ အတွက် ဖွင့်မဲ စနစ် ဖြင့် ကျင်းပမည် ဖြစ် ကြောင်း ရွေးကောက်ပွဲ တွင် အခိုင်အမာပြောဆိုခဲ့ပါသည် ။\n",
      "TARGET: When a discussion into how the speaker should be decided started, lawmakers with Raila Odinaga's opposition Orange Democratic Movement insisted the election for speaker be held by open ballot; Mr. Kibaki's supporters demanded the vote be secret.\n",
      "PREDICTED: The European Union has rejected the decision of by the European Union , and the European Union , , which has been by the European Union , and the European Union , and the European Union ' s government ' s government .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: သူ့ စနစ် တွင် အခြား မည်သည့် အဆိပ်များ ရှိသည် ကို မသိသေးပါ ။\n",
      "TARGET: It is unknown if there were any other poisons in his system.\n",
      "PREDICTED: He is not known as the of the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ထို့အပြင် ၊ ကျိုးကျနေသော သစ်ကိုင်းများ အားလုံး မိနစ် ၃၀ ကြာချိန် အတွင်း ရှင်းလင်း ရန် မျှော်လင့်သည် ။\n",
      "TARGET: Additionally, all running branches are expected to be running with 30 minute delays.\n",
      "PREDICTED: In addition , there are approximately 30 miles per hour and .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  16: 100%|███████████████████████████████████████████████████████████████████████████████████| 4521/4521 [20:52<00:00,  3.61it/s, loss=3.785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ကုလသမဂ္ဂ က လူ တစ် သန်း ဟာ အိုးမဲ့အိမ်မဲ့ ဖြစ်ကြတယ် ၊ ဒါမှမဟုတ် အခြား အချို့ နည်းလမ်း တွင် အကူအညီ လိုအပ်နေတယ် လို့ အစီရင်ခံခဲ့သည် ။\n",
      "TARGET: The United Nations has reported that one million people are homeless, or in need of help in some other way.\n",
      "PREDICTED: The UN has confirmed that they had been aware of the , but that it is unlikely that the situation was .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: အဆိုပါ ဖိနပ် နှင့် ဥမင် သည် သုတေသနပြု ဖို့ ရန် ဆက်လက်ရှိပါလိမ့်မယ် ။\n",
      "TARGET: The shoe and the cave will continue to be researched.\n",
      "PREDICTED: The storm will be expected to be operating in the region .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: သူ သည် စစ်ပွဲ-ဆန့်ကျင်သော ရုပ်ရှင်ကား မှ အမှန်တကယ် မြင်ကွင်း တစ်ခု ဖြစ် သော ၊ မုဒိမ်းကျင့်သည့် ရုပ်ရှင် အတု များ ကဲ့သို့ ပစ္စည်း များ ကို လက်ခံခြင်း မှတဆင့် တစ်ဦးတည်း အစွန်းရောက်ခြင်း ရရှိ ရန် ပြောကြားခဲ့သည် ။\n",
      "TARGET: He was said to have radicalised alone through access to material such as the fake rape film, which was actually a scene from the anti-war movie .\n",
      "PREDICTED: He was also the first time that the two - thirds majority in the war , and the government has not been by the government and has not been by the government .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: “ ဤအရာ သည် လက်ရှိ ဥပဒေပြုလွှတ်တော် အတွက် ထို နည်းလမ်းများ ကို ဖျက်သိမ်း ရန် နှင့် ၎င်း တို့ကို အတည်ပြုပြုဌါန်းသည့်ဥပဒေ တစ်ခု ဆီသို့ ပြောင်းလဲ ရန် အချိန် ဖြစ် ပြီး ၊ တရားရုံး ရဲ့ လှုပ်ရှားမှု တစ်ခု သည် အကောင်းဆုံး လေးစားလိုက်နာခြင်း ကို ပေးလိမ့်မည် ။ ”\n",
      "TARGET: \"It’s now time for Congress to sign-off on these procedures and turn them into a statute, a move the courts will give great deference to.\"\n",
      "PREDICTED: \" This is a very important way that these law will be and will be in order to provide law and provide a law ,\" said .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ၎င်း အပြင် ၊ ပတ်ဒီလ်လာ အား “ အကြမ်းဖက် ဂျီဟတ် ” တွင် လေကျင့် ရန် ပြည်ပ သို့ ခရီးသွားနေခြင်း ၊ အကြမ်းဖက်သမားများ ထံ သို့ ငွေ ၊ ရုပ် ပိုင်း ဆိုင်ရာ ပိုင်ဆိုင်မှုများ နှင့် ၊ ဂျီဟတ် အုပ်စုများ ထံ သို့ တပ်သား သစ်များ နှင့် ၊ နိုင်ငံခြားသား တစ်ဉီးစီ ကို သတ် ရန် လျှို့ဝှက် ကြံစည်းခြင်း တို့ အား ပေးပို့ခြင်း ဖြင့် ပစ္စည်း ထောက်ပံ့မှု စီစဉ်ပေးနေခြင်း တို့ ဖြင့် စွဲချက်တင်ခဲ့သည် ။\n",
      "TARGET: Instead, Padilla is charged with travelling abroad to train in \"violent Jihad\", providing material support to terrorists by sending money, physical assets, and new recruits to Jihadi groups, and conspiring to murder individuals overseas .\n",
      "PREDICTED: In addition , the , and , the , and , and both times the of and .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  17: 100%|███████████████████████████████████████████████████████████████████████████████████| 4521/4521 [26:04<00:00,  2.89it/s, loss=3.633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: သို့သော် ၎င်း သည် ၎င်းတို့ ၏ အကျိုးအမြတ် ကို တိုးတက် ရန် ကြိုးပမ်းမှု တစ်ခု ကို ၎င်း ကဲ့သို့ ပြုလုပ်ခဲ့ သည့် နိုကီရာ ကုမ္ပဏီ က ပထမဆုံး အကြိမ် မဟုတ်ပါဘူး ။\n",
      "TARGET: But this is not the first time Nokia has made an attempt like this to increase its revenue.\n",
      "PREDICTED: But it would not be used to reduce its own content in its own or form of companies they will not be .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ဒေသ ဆိုင်ရာ ရဲတပ်ဖွဲ့ အကြီးအကဲ ဟားဘတ် ကဲမ်ဘာ က : “ ရက်စက်သော လူသတ်မှု ၏ အဓိက စီစဉ်သူ လို့ ယုံကြည်ခဲ့သည့် သံသယရှိသူ ကို ကင်ညာ နယ်စပ် မြို့ တာဗက်တာ ၌ ပုန်းရှောင်နေသည် ကို ရဲတပ်ဖွဲ့ က ဖမ်းဆီးခဲ့ကြောင်း ယခုနေ့ တွင် ကြေငြာခဲ့သည် ။\n",
      "TARGET: Regional police chief Herbert Khaemba anounced today: \"The suspect believed to be the main organiser of the brutal murder was arrested by the police at the Kenyan border town of Taveta in hiding.\n",
      "PREDICTED: The driver of the vehicle was found at the scene as a result of his vehicle at the in the in the city of .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ဇင်ဘာဘွေ နိုင်ငံ မှ သဘာဝ တောရိုင်း များ နှင့် တိရစ္ဆာန် များ ထိန်းသိမ်း စောင့်ရှောက်ရေး တာဝန်ခံ အရာရှိ တစ်ဦး က နိုင်ငံတကာ နှင့် ဆိုင်သော မှောင်ခို ဂိုဏ်း တစ်ဂိုဏ်း ကို အပြစ်တင် လျက် ၎င်း တို့ နိုင်ငံ ၌ တိရစ္ဆာန် များ ကို တရားမဝင် ဖမ်းဆီး သတ်ဖြတ်ခြင်း ၏ ပမာဏ သည် ယခု နှစ် တွင် အဆမတန် များပြားလာခဲ့သည် ဟု ယနေ့ ပြောကြားခဲ့သည် ။\n",
      "TARGET: A wildlife official in Zimbabwe said today that the amount of animal poaching in the country has increased substantially this year, blaming an international crime syndicate.\n",
      "PREDICTED: The World Health Organisation said that a storm in the country has been involved in its in the country , which has been placed in a in the country where it is reported to be a of in the country with its .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \" စုံစမ်းစစ်ဆေးချက် များ အဆိုအရ ၊ အကြမ်းဖက်မှု ၏ အပြုအမူ သည် ပထမဆုံး နှင့် အစောဆုံး နိုင်ငံခြားသား များ ဆန့်ကျင်မှု ကို ရည်ရွယ်ထားခြင်း ဖြစ်သည် ။ \"\n",
      "TARGET: \"According to investigators, the act of terror was first and foremost aimed against foreign citizens.\"\n",
      "PREDICTED: \" In the investigation , the is to be by the of the and the of the .\"\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: “ အကြမ်းဖက် ဆောင်ရွက်မှု သည် နိုင်ငံတကာ လေဆိပ် ခန်းမ ….. တွင် ကျူးလွန်ခဲ့သည် ဟု အထူးသဖြင့် ကျွန်ုပ် က ၄င်း သည် မတော်တဆဖြစ်ပွားခြင်း တစ်ခု လို့ ယူဆ လို့ မရ ခြင်း ဖြင့် ဖြစ်နိုင်သည် ဟု မှတ်ချက်ချချင်ပါသည် လို့ ” မာကင် က ထပ်ပြောခဲ့သည်\n",
      "TARGET: Markin added \"I would especially like to note that it was by no means an accident that the act of terror was committed in the international arrivals hall…\"\n",
      "PREDICTED: \" If the bill ... is not clear whether they would be allowed to be allowed ... because they would be a with the United States ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  18: 100%|███████████████████████████████████████████████████████████████████████████████████| 4521/4521 [18:21<00:00,  4.10it/s, loss=3.902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: သူ၏ သေဆုံးမှု နှင့်ပတ်သတ်၍ သူတို့ မသိခဲ့ဘူးလို့ ပါကစ္စတန် အစိုးရ က ပြောခဲ့သည် ။\n",
      "TARGET: The Government of Pakistan said they did not know about his death.\n",
      "PREDICTED: His and were not allowed to stop them .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ရာဘင် နှင့် စပ်လျဉ်း၍ ၊ ကလင်တန် က “ ကျွန်တော် သူ့ ကို အလွန် နှစ်သက်ပါတယ် ၊ ပြီးတော့ နိုင်ငံရေးသမား တစ်ဦး အား တိုင်းပြည် အုပ်ချုပ်တတ်သူ တစ်ဦး ၊ စစ်သား တစ်ယောက် ဖြစ်နေခြင်း မှ ငြိမ်းချမ်းရေး ဖော်ဆောင်သူ တစ်ယောက် အဖြစ် သို့ ၊ ပြောင်းလဲနိုင် ရန် သူ့ ရဲ့ စွမ်းရည် ကို ကျွန်တော် အံ့ဩခဲ့ရပါတယ် ” ဟု ၊ ပြောကြားခဲ့သည် ။\n",
      "TARGET: Regarding Rabin, Clinton said, \"I loved him very much, and I was in awe of his ability to move from being a soldier to being a peacemaker, a politician to a statesman\".\n",
      "PREDICTED: , a of the , said : \" We have a very of hope and the of the best people , they have to do that , but not the fact that we are not the subject .\"\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: သူ သည် ဝါရှင်တန် ၊ ဒီစီ သို့ အပြန် လမ်း တွင် အဲ ဖို့စ် ဝန်း ပေါ်မှ တင်မိုသီ ရှရစ်ဗာ ဆီ တယ်လီဖုန်း ခေါ် ခဲ့ သည် ။\n",
      "TARGET: He phoned Timothy Shriver from Air Force One on the way back to Washington, D.C..\n",
      "PREDICTED: He was arrested by a military officer in Washington , Washington , Washington , Washington D . C .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: သူ သည် ၃၁ ရက် နေ့ တွင် ဗုံးခွဲသူ တစ်ယောက် ဆင်ခဲ့သော ကိရိယာ အချိန်မတန်ပဲ ပေါက်ကွဲခြင်း နောက်ပိုင်း သူ သေဆုံးခဲ့ သော ၊ မော်စကို ဟိုတယ် တစ်ခု မှာ ပေါက်ကွဲမှု တစ်ခု ဖြစ် ရန် သူတို့ ကို ချိတ်ဆက်ခဲ့တယ် ။\n",
      "TARGET: He linked them to an explosion on the 31st in a Moscow hotel, in which a bomber died after the device he was building went off prematurely.\n",
      "PREDICTED: He was attacked when he was attacked by a suicide bombing his , killing 13 people .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: အများဆုံး မြေးတွေ နဲ့အတူ သက်ရှိ ထင်ရှား ရှိတဲ့ စုံတွဲများ အတွက် လက်ရှိ ဂရင်းနစ် စံချိန် တွင် မ ရှိသေးသော ကာလ အတွင်း ဖြစ်သည် ၊ ထို စုံတွဲ သည် စံချိန် တင် ရန် အရမ်း ကောင်းမွန် လိမ့်မည် ဟု ဂရင်းနစ် ကမ္ဘာ မှတ်တမ်းများ မှ တာဝန်ရှိသူများ ပြောကြားသည် ။\n",
      "TARGET: While there is no existing record with Guinness for alive couples with the most grandchildren, Guinness World Records authorities say the couple very well might hold the record.\n",
      "PREDICTED: in the manner of the most parts of the country have been in the of the and the , but the most recent are not the most likely to be for the hospital at the time .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch  19: 100%|███████████████████████████████████████████████████████████████████████████████████| 4521/4521 [18:51<00:00,  4.00it/s, loss=3.335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: အမျိုးသမီး များ ပွဲစဉ် တွင် ၊ ဂျင်နီ ဖေး သည် မီချဲလ် အိန်းစ်ဝသ် ကို ဦးဆောင် ၍ ၄ နာရီ ၁ မိနစ် ၅၃ စက္ကန့် အချိန် တွင် အနိုင်ရခဲ့သည် ။\n",
      "TARGET: In the women's event, Jenny Fay won in a time of 4:01:53 ahead of Michelle Ainsworth.\n",
      "PREDICTED: The women , who were aged 47 , were aged 47 and a woman who was aged 47 .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: သူ သည် လူသတ်မှု ၊ ပြန်ပေးဆွဲမှု ၊ အကြမ်းဖက် အုပ်စုများ ကို ကူညီပေးမှု နှင့် အမေရိကန် ပြည်ထောင်စု အပြင် ဘက် ရှိ အမေရိကန် ပြည်ထောင်စု နိုင်ငံတော်များ ကို ဆန့်ကျင်သော အခြား လုပ်ဆောင်ချက်များ ကျူးလွန် ရန် ပူးပေါင်းကြံစည်မှု ဖြင့် စွဲချက်တင်ခံခဲ့ရသည် ။\n",
      "TARGET: He is charged with conspiracy to commit murder, kidnapping, aiding terrorist groups and other acts against US nationals outside the United States.\n",
      "PREDICTED: He has been criticized for the United States Senate in Iraq for several years , including US citizens and the United States to send a message to Iraq for foreign exchange .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: အိုက်စ်လန်ဒစ် ကုမ္ပဏီများ သည် လွန်ခဲ့တဲ့ နှစ်အနည်းငယ် ကတည်းက ဒိန်းမတ် မှာ အကြီးအကျယ် ရင်းနှီးမြှုပ်နှံမှုပြုလုပ်ခဲ့ကြသည် ။\n",
      "TARGET: Icelandic companies have been investing heavily in Denmark for the last few years.\n",
      "PREDICTED: The have been made in the second half after the Indian Ocean started with the .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: “ ကျွန်ုပ် သည် နိုင်ငံ ကို တည်ဆောက် ရန် ပြုပြင်မွမ်းမံရေး နှင့်အတူ ဆက်လက်ဆောင်ရွက်သွားနိုင် ရန် နှင့် အများပြည်သူ ထိတွေ့ခံစားရရှိနိုင် သော စီးပွားရေး တိုးတက်မှု ကို အထောက်အကူပေးနိုင် ရန် ကျွန်ုပ်၏ တာဝန် ကို ကျေပွန်အောင် ဆောင်ရွက်ချင်ပါသည် ။ “\n",
      "TARGET: \"I want to fulfill my responsibility to proceed with reform to build the nation and promote economic growth that the people can feel.\"\n",
      "PREDICTED: \" I am convinced that I am willing to have the right to stop my country and the of the American citizens to stop the American people to stop my face of people .\"\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: “ ခင်ဗျား က ဝင်းဒိုး ပရိုဂရမ် တစ်ခု ဝယ်မယ် ဆိုရင် ၊ အဲဒါက ဝင်းဒိုး ကွန်ပျူတာ တစ်ခု ပေါ်မှာပဲ ဖတ်မယ် ဆိုတာကို ခင်ဗျား သိ ပြီး ၊ လင်းနက်စ် ကလည်း အဲ့ဒီ့ နည်းလမ်း အတိုင်းပဲ လုပ်ဆောင် ဖို့ လိုပါတယ် ။\n",
      "TARGET: \"If you buy a Windows program, you know it will run on a Windows computer, and Linux needs to work the same way.\n",
      "PREDICTED: \" The Foundation is the website , which leads to a website website , a Christian website .\n"
     ]
    }
   ],
   "source": [
    "def greedy_decode(model, source, source_mask,src_tokenizer,tgt_tokenizer,tgt_seq_len,device):\n",
    "    sos_idx=src_tokenizer.token_to_id('[SOS]')\n",
    "    eos_idx=src_tokenizer.token_to_id('[EOS]')\n",
    "    # precompute the encoder output and reuse it for every token we get from the decoder\n",
    "    encoder_output=model.encode(source,source_mask)\n",
    "\n",
    "    #Initialize the decoder input with the sos token\n",
    "    decoder_input=torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1)==tgt_seq_len:\n",
    "            break\n",
    "        \n",
    "        decoder_mask=causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "        decoder_output=model.decode(encoder_output,source_mask,decoder_input,decoder_mask)\n",
    "        prob=model.project(decoder_output[:,-1])\n",
    "        _,next_word=torch.max(prob,dim=-1)\n",
    "        decoder_input=torch.cat([decoder_input,torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)],dim=1)\n",
    "\n",
    "        if next_word==eos_idx:\n",
    "            break\n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "  \n",
    "def run_validation(model,validation_ds,src_tokenizer,tgt_tokenizer,tgt_seq_len,device,print_msg,global_state,writer,num_examples=5):\n",
    "    model.eval()\n",
    "    count=0\n",
    "    source_texts=[]\n",
    "    expected = []\n",
    "    predicted = []\n",
    "    console_width = 80 #size of the control window(defauly value)\n",
    "\n",
    "    tgt_seq_len=config['tgt_seq_len']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count+=1\n",
    "            encoder_input=batch['encoder_input'].to(device)\n",
    "            encoder_mask=batch['encoder_mask'].to(device)\n",
    "\n",
    "\n",
    "            assert encoder_input.size(0)==1, \"Batch size must be 1 for validation\"\n",
    "            model_out=greedy_decode(model,encoder_input,encoder_mask,src_tokenizer,tgt_tokenizer,tgt_seq_len,device)\n",
    "\n",
    "            source_text=batch['src_text'][0]\n",
    "            target_text=batch['tgt_text'][0]\n",
    "            model_out_text=tgt_tokenizer.decode(model_out.detach().cpu().numpy())\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"SOURCE: {source_text}\")\n",
    "            print_msg(f\"TARGET: {target_text}\")\n",
    "            print_msg(f\"PREDICTED: {model_out_text}\")\n",
    "\n",
    "            if count==num_examples:\n",
    "                break\n",
    "   \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "def get_ds(config):\n",
    "    ds_name=config['ds_name']\n",
    "    ds_subset=config['ds_subset']\n",
    "    ds_raw=load_dataset(ds_name, ds_subset)\n",
    "    train_ds, test_ds, val_ds= clean_dataset(ds_raw)\n",
    "\n",
    "    src_tokenizer=create_burmese_tokenizer(train_ds,config)\n",
    "    tgt_tokenizer=create_english_tokenizer(train_ds,config)\n",
    "\n",
    "    train_ds1=BilingualDataset(train_ds,src_tokenizer,tgt_tokenizer,config['src_lang'],config['tgt_lang'],config['src_seq_len'],config['tgt_seq_len'])\n",
    "    test_ds=BilingualDataset(test_ds,src_tokenizer,tgt_tokenizer,config['src_lang'],config['tgt_lang'],config['src_seq_len'],config['tgt_seq_len'])\n",
    "    val_ds=BilingualDataset(val_ds,src_tokenizer,tgt_tokenizer,config['src_lang'],config['tgt_lang'],config['src_seq_len'],config['tgt_seq_len'])\n",
    "    \n",
    "    max_len_src=0\n",
    "    max_len_tgt=0\n",
    "\n",
    "    for item in train_ds['translation']:\n",
    "       \n",
    "        src_ids=src_tokenizer.encode(item['burmese']).ids\n",
    "        tgt_ids=tgt_tokenizer.encode(item['english']).ids\n",
    "        max_len_src=max(max_len_src,len(src_ids))\n",
    "        max_len_tgt=max(max_len_tgt,len(tgt_ids))\n",
    "\n",
    "    print(f'Maximum length of source sentence: {max_len_src}')\n",
    "    print(f'Maximum length of target sentence: {max_len_tgt}')\n",
    "   \n",
    "\n",
    "\n",
    "    train_dataloader=DataLoader(train_ds1,batch_size=config['batch_size'],shuffle=True)\n",
    "    test_dataloader=DataLoader(test_ds,batch_size=config['batch_size'],shuffle=True)\n",
    "    val_dataloader=DataLoader(val_ds,batch_size=1,shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader, val_dataloader, src_tokenizer, tgt_tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def get_model(config,src_vocab_size,tgt_vocab_size):\n",
    "    model=build_transformer(src_vocab_size, tgt_vocab_size, config['src_seq_len'], config['tgt_seq_len'], config['d_model'], config['N'], config['h'], config['dropout'], config['d_ff'])\n",
    "    return model\n",
    "    \n",
    "def train_model(config):\n",
    "    device=torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    Path(config['model_folder']).mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "    train_dataloader, test_dataloader, val_dataloader, src_tokenizer, tgt_tokenizer=get_ds(config)\n",
    "    model=get_model(config,src_tokenizer.get_vocab_size(),tgt_tokenizer.get_vocab_size()).to(device)\n",
    "    writer=SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=config['lr'],eps=1e-9)\n",
    "\n",
    "    initial_epoch=0\n",
    "    global_step=0\n",
    "\n",
    "    if config['preload']:\n",
    "        model_filename=get_weights_file_path(config,config['preload'])\n",
    "        state=torch.load(model_filename)\n",
    "        initial_epoch=state['epoch']+1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step=state['global_step']\n",
    "    \n",
    "    loss_fn=nn.CrossEntropyLoss(ignore_index=src_tokenizer.token_to_id('[PAD]'),label_smoothing=0.1).to(device)\n",
    "\n",
    "    for epoch in range(initial_epoch,config['num_epochs']):\n",
    "        \n",
    "        batch_iterator=tqdm(train_dataloader,desc=f\"Training epoch {epoch : 02d}\")\n",
    "        for batch in batch_iterator:\n",
    "            model.train()\n",
    "            encoder_input=batch['encoder_input'].to(device) #(batch_size,seq_len)\n",
    "            decoder_input=batch['decoder_input'].to(device) #(batch_size,seq_len)\n",
    "            encoder_mask=batch['encoder_mask'].to(device) #(batch_size,1,1,seq_len)\n",
    "            decoder_mask=batch['decoder_mask'].to(device) #(batch_size,1,seq_len,seq_len)\n",
    " \n",
    "        \n",
    "\n",
    "            encoder_output=model.encode(encoder_input,encoder_mask) #(batch_size,seq_len,d_model)\n",
    "            decoder_output=model.decode(encoder_output,encoder_mask,decoder_input,decoder_mask) #(batch_size,seq_len,d_model)\n",
    "            proj_output=model.project(decoder_output) #(batch_size,seq_len,tgt_vocab_size)\n",
    "\n",
    "            label=batch['label'].to(device) #(batch_size,seq_len)\n",
    "            loss=loss_fn(proj_output.view(-1,tgt_tokenizer.get_vocab_size()),label.view(-1)) #whats happpening here?\n",
    "            batch_iterator.set_postfix({\"loss\":f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            #log the loss to tensorboard\n",
    "            writer.add_scalar('train_loss',loss.item(),global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            loss.backward() #backpropagate the loss\n",
    "\n",
    "            \n",
    "            optimizer.step() #update the weights\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            global_step+=1\n",
    "\n",
    "        run_validation(model,val_dataloader,src_tokenizer,tgt_tokenizer,config,device,lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "\n",
    "        \n",
    "        model_filename=get_weights_file_path(config,f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step,\n",
    "        },model_filename)\n",
    "\n",
    "\n",
    "\n",
    "config=get_config()\n",
    "train_model(config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in current directory: ['.config', 'Music', 'logistic_net_graph', 'webots-R2023b.dmg', '.cursor', '.zprofile.pysave', '.codegpt', '.condarc', '.docker', 'component_loadings.png', 'Untitled1.ipynb', '.DS_Store', 'correlation_matrix.png', '.CFUserTextEncoding', 'test', '.llama', 'variable_variances.png', '.xonshrc', 'anaconda_projects', 'Untitled.ipynb', '.zshrc', 'weights', '.streamlit', '.local', '.psql_history', 'Pictures', 'raw_data2.png', '.zprofile', 'kmeans_clusters.png', 'Postman', '.zsh_history', 'Untitled2.ipynb', '.ipython', 'pairplot.png', 'Desktop', 'Library', '.matplotlib', '.lesshst', 'component_projections.png', '.pgadmin', 'tokenizer_my', 'Public', '.idlerc', 'Untitled.R', '.tcshrc', '.anaconda', '.ssh', 'Movies', 'Applications', 'variable_distributions.png', '.Rapp.history', '.Trash', 'KMNIST', '.ipynb_checkpoints', '.jupyter', '.npm', 'Documents', '.anydesk', '.bash_profile', 'Downloads', '.python_history', '.continuum', '.cache', 'runs', '.ollama', '.bash_history', '.zsh_sessions', 'tokenizer_en', '.conda']\n",
      "Model files found: []\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/jithuazeez\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'weights' exists\n",
      "Files in model directory: ['tmodel_15.pt', 'tmodel_05.pt', 'tmodel_11.pt', 'tmodel_01.pt', 'tmodel_10.pt', 'tmodel_00.pt', 'tmodel_14.pt', 'tmodel_04.pt', 'tmodel_19.pt', 'tmodel_09.pt', 'tmodel_18.pt', 'tmodel_08.pt', 'tmodel_13.pt', 'tmodel_03.pt', 'tmodel_17.pt', 'tmodel_07.pt', 'tmodel_16.pt', 'tmodel_06.pt', 'tmodel_12.pt', 'tmodel_02.pt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model_folder = config['model_folder']\n",
    "if os.path.exists(model_folder):\n",
    "    print(f\"Directory '{model_folder}' exists\")\n",
    "    print(\"Files in model directory:\", os.listdir(model_folder))\n",
    "else:\n",
    "    print(f\"Directory '{model_folder}' does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burmese character length statistics:\n",
      "50th percentile: 177.0\n",
      "75th percentile: 235.0\n",
      "90th percentile: 303.0\n",
      "95th percentile: 348.0\n",
      "98th percentile: 410.0\n",
      "99th percentile: 460.0\n",
      "\n",
      "English word length statistics:\n",
      "50th percentile: 21.0\n",
      "75th percentile: 28.0\n",
      "90th percentile: 36.0\n",
      "95th percentile: 41.0\n",
      "98th percentile: 48.0\n",
      "99th percentile: 53.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "burmese_lengths = [len(list(item['translation']['burmese'])) for item in train_dataset]\n",
    "# For English word lengths\n",
    "english_lengths = [len(item['translation']['english'].split()) for item in train_dataset]\n",
    "\n",
    "# Calculate percentiles\n",
    "percentiles = [50, 75, 90, 95, 98, 99]\n",
    "burmese_stats = {p: np.percentile(burmese_lengths, p) for p in percentiles}\n",
    "english_stats = {p: np.percentile(english_lengths, p) for p in percentiles}\n",
    "\n",
    "print(\"Burmese character length statistics:\")\n",
    "for p, v in burmese_stats.items():\n",
    "    print(f\"{p}th percentile: {v:.1f}\")\n",
    "\n",
    "print(\"\\nEnglish word length statistics:\")\n",
    "for p, v in english_stats.items():\n",
    "    print(f\"{p}th percentile: {v:.1f}\")\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(burmese_lengths, bins=50)\n",
    "plt.title('Burmese Character Length Distribution')\n",
    "plt.xlabel('Number of characters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(burmese_stats[95], color='r', linestyle='--', label='95th percentile')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(english_lengths, bins=30)\n",
    "plt.title('English Word Length Distribution')\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(english_stats[95], color='r', linestyle='--', label='95th percentile')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 1 ===\n",
      "BURMESE TOKENIZATION (Character-level):\n",
      "Original text: ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂၀၀၇ခုနှစ် ရပ်ဘီ ကမ္ဘာ့ ဖလား တွင် အီတလီ သည် ပေါ်တူဂီ ကို ၃၁-၅ ဂိုး ဖြင့် ရေကူးကန် စီ တွင် ရှုံးနိမ့်သွားပါသည် ။\n",
      "Tokenized into 156 tokens:\n",
      "  1. 'ပ' → ID: 17\n",
      "  2. 'ြ' → ID: 20\n",
      "  3. 'င' → ID: 12\n",
      "  4. '်' → ID: 5\n",
      "  5. 'သ' → ID: 15\n",
      "  6. 'စ' → ID: 22\n",
      "  7. '်' → ID: 5\n",
      "  8. 'န' → ID: 16\n",
      "  9. 'ိ' → ID: 11\n",
      "  10. 'ု' → ID: 8\n",
      "  11. 'င' → ID: 12\n",
      "  12. '်' → ID: 5\n",
      "  13. 'င' → ID: 12\n",
      "  14. 'ံ' → ID: 33\n",
      "  15. ' ' → ID: 4\n",
      "  16. 'ပ' → ID: 17\n",
      "  17. 'ါ' → ID: 36\n",
      "  18. 'ရ' → ID: 21\n",
      "  19. 'ီ' → ID: 29\n",
      "  20. 'မ' → ID: 13\n",
      "  21. 'ြ' → ID: 20\n",
      "  22. 'ိ' → ID: 11\n",
      "  23. 'ု' → ID: 8\n",
      "  24. '့' → ID: 18\n",
      "  25. ' ' → ID: 4\n",
      "  26. 'ပ' → ID: 17\n",
      "  27. 'ါ' → ID: 36\n",
      "  28. '့' → ID: 18\n",
      "  29. 'ဒ' → ID: 40\n",
      "  30. 'က' → ID: 9\n",
      "  31. '်' → ID: 5\n",
      "  32. 'စ' → ID: 22\n",
      "  33. '်' → ID: 5\n",
      "  34. ' ' → ID: 4\n",
      "  35. 'ပ' → ID: 17\n",
      "  36. 'ရ' → ID: 21\n",
      "  37. 'င' → ID: 12\n",
      "  38. '်' → ID: 5\n",
      "  39. '့' → ID: 18\n",
      "  40. 'စ' → ID: 22\n",
      "  41. 'က' → ID: 9\n",
      "  42. '်' → ID: 5\n",
      "  43. ' ' → ID: 4\n",
      "  44. '၌' → ID: 72\n",
      "  45. ' ' → ID: 4\n",
      "  46. '၂' → ID: 51\n",
      "  47. '၀' → ID: 47\n",
      "  48. '၀' → ID: 47\n",
      "  49. '၇' → ID: 70\n",
      "  50. 'ခ' → ID: 23\n",
      "  51. 'ု' → ID: 8\n",
      "  52. 'န' → ID: 16\n",
      "  53. 'ှ' → ID: 24\n",
      "  54. 'စ' → ID: 22\n",
      "  55. '်' → ID: 5\n",
      "  56. ' ' → ID: 4\n",
      "  57. 'ရ' → ID: 21\n",
      "  58. 'ပ' → ID: 17\n",
      "  59. '်' → ID: 5\n",
      "  60. 'ဘ' → ID: 41\n",
      "  61. 'ီ' → ID: 29\n",
      "  62. ' ' → ID: 4\n",
      "  63. 'က' → ID: 9\n",
      "  64. 'မ' → ID: 13\n",
      "  65. '္' → ID: 46\n",
      "  66. 'ဘ' → ID: 41\n",
      "  67. 'ာ' → ID: 7\n",
      "  68. '့' → ID: 18\n",
      "  69. ' ' → ID: 4\n",
      "  70. 'ဖ' → ID: 35\n",
      "  71. 'လ' → ID: 25\n",
      "  72. 'ာ' → ID: 7\n",
      "  73. 'း' → ID: 6\n",
      "  74. ' ' → ID: 4\n",
      "  75. 'တ' → ID: 14\n",
      "  76. 'ွ' → ID: 26\n",
      "  77. 'င' → ID: 12\n",
      "  78. '်' → ID: 5\n",
      "  79. ' ' → ID: 4\n",
      "  80. 'အ' → ID: 19\n",
      "  81. 'ီ' → ID: 29\n",
      "  82. 'တ' → ID: 14\n",
      "  83. 'လ' → ID: 25\n",
      "  84. 'ီ' → ID: 29\n",
      "  85. ' ' → ID: 4\n",
      "  86. 'သ' → ID: 15\n",
      "  87. 'ည' → ID: 27\n",
      "  88. '်' → ID: 5\n",
      "  89. ' ' → ID: 4\n",
      "  90. 'ပ' → ID: 17\n",
      "  91. 'ေ' → ID: 10\n",
      "  92. 'ါ' → ID: 36\n",
      "  93. '်' → ID: 5\n",
      "  94. 'တ' → ID: 14\n",
      "  95. 'ူ' → ID: 34\n",
      "  96. 'ဂ' → ID: 45\n",
      "  97. 'ီ' → ID: 29\n",
      "  98. ' ' → ID: 4\n",
      "  99. 'က' → ID: 9\n",
      "  100. 'ိ' → ID: 11\n",
      "  101. 'ု' → ID: 8\n",
      "  102. ' ' → ID: 4\n",
      "  103. '၃' → ID: 64\n",
      "  104. '၁' → ID: 49\n",
      "  105. '-' → ID: 61\n",
      "  106. '၅' → ID: 60\n",
      "  107. ' ' → ID: 4\n",
      "  108. 'ဂ' → ID: 45\n",
      "  109. 'ိ' → ID: 11\n",
      "  110. 'ု' → ID: 8\n",
      "  111. 'း' → ID: 6\n",
      "  112. ' ' → ID: 4\n",
      "  113. 'ဖ' → ID: 35\n",
      "  114. 'ြ' → ID: 20\n",
      "  115. 'င' → ID: 12\n",
      "  116. '်' → ID: 5\n",
      "  117. '့' → ID: 18\n",
      "  118. ' ' → ID: 4\n",
      "  119. 'ရ' → ID: 21\n",
      "  120. 'ေ' → ID: 10\n",
      "  121. 'က' → ID: 9\n",
      "  122. 'ူ' → ID: 34\n",
      "  123. 'း' → ID: 6\n",
      "  124. 'က' → ID: 9\n",
      "  125. 'န' → ID: 16\n",
      "  126. '်' → ID: 5\n",
      "  127. ' ' → ID: 4\n",
      "  128. 'စ' → ID: 22\n",
      "  129. 'ီ' → ID: 29\n",
      "  130. ' ' → ID: 4\n",
      "  131. 'တ' → ID: 14\n",
      "  132. 'ွ' → ID: 26\n",
      "  133. 'င' → ID: 12\n",
      "  134. '်' → ID: 5\n",
      "  135. ' ' → ID: 4\n",
      "  136. 'ရ' → ID: 21\n",
      "  137. 'ှ' → ID: 24\n",
      "  138. 'ု' → ID: 8\n",
      "  139. 'ံ' → ID: 33\n",
      "  140. 'း' → ID: 6\n",
      "  141. 'န' → ID: 16\n",
      "  142. 'ိ' → ID: 11\n",
      "  143. 'မ' → ID: 13\n",
      "  144. '်' → ID: 5\n",
      "  145. '့' → ID: 18\n",
      "  146. 'သ' → ID: 15\n",
      "  147. 'ွ' → ID: 26\n",
      "  148. 'ာ' → ID: 7\n",
      "  149. 'း' → ID: 6\n",
      "  150. 'ပ' → ID: 17\n",
      "  151. 'ါ' → ID: 36\n",
      "  152. 'သ' → ID: 15\n",
      "  153. 'ည' → ID: 27\n",
      "  154. '်' → ID: 5\n",
      "  155. ' ' → ID: 4\n",
      "  156. '။' → ID: 39\n",
      "\n",
      "Character by character verification:\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character 'ြ' → Token 'ြ'\n",
      "  Character 'င' → Token 'င'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'သ' → Token 'သ'\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'န' → Token 'န'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character 'င' → Token 'င'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'င' → Token 'င'\n",
      "  Character 'ံ' → Token 'ံ'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character 'ါ' → Token 'ါ'\n",
      "  Character 'ရ' → Token 'ရ'\n",
      "  Character 'ီ' → Token 'ီ'\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ြ' → Token 'ြ'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character '့' → Token '့'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character 'ါ' → Token 'ါ'\n",
      "  Character '့' → Token '့'\n",
      "  Character 'ဒ' → Token 'ဒ'\n",
      "  Character 'က' → Token 'က'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character 'ရ' → Token 'ရ'\n",
      "  Character 'င' → Token 'င'\n",
      "  Character '်' → Token '်'\n",
      "  Character '့' → Token '့'\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character 'က' → Token 'က'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character '၌' → Token '၌'\n",
      "  Character ' ' → Token ' '\n",
      "  Character '၂' → Token '၂'\n",
      "  Character '၀' → Token '၀'\n",
      "  Character '၀' → Token '၀'\n",
      "  Character '၇' → Token '၇'\n",
      "  Character 'ခ' → Token 'ခ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character 'န' → Token 'န'\n",
      "  Character 'ှ' → Token 'ှ'\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ရ' → Token 'ရ'\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'ဘ' → Token 'ဘ'\n",
      "  Character 'ီ' → Token 'ီ'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'က' → Token 'က'\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character '္' → Token '္'\n",
      "  Character 'ဘ' → Token 'ဘ'\n",
      "  Character 'ာ' → Token 'ာ'\n",
      "  Character '့' → Token '့'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ဖ' → Token 'ဖ'\n",
      "  Character 'လ' → Token 'လ'\n",
      "  Character 'ာ' → Token 'ာ'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'ွ' → Token 'ွ'\n",
      "  Character 'င' → Token 'င'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'အ' → Token 'အ'\n",
      "  Character 'ီ' → Token 'ီ'\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'လ' → Token 'လ'\n",
      "  Character 'ီ' → Token 'ီ'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'သ' → Token 'သ'\n",
      "  Character 'ည' → Token 'ည'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character 'ေ' → Token 'ေ'\n",
      "  Character 'ါ' → Token 'ါ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'ူ' → Token 'ူ'\n",
      "  Character 'ဂ' → Token 'ဂ'\n",
      "  Character 'ီ' → Token 'ီ'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'က' → Token 'က'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character ' ' → Token ' '\n",
      "  Character '၃' → Token '၃'\n",
      "  Character '၁' → Token '၁'\n",
      "  Character '-' → Token '-'\n",
      "  Character '၅' → Token '၅'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ဂ' → Token 'ဂ'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ဖ' → Token 'ဖ'\n",
      "  Character 'ြ' → Token 'ြ'\n",
      "  Character 'င' → Token 'င'\n",
      "  Character '်' → Token '်'\n",
      "  Character '့' → Token '့'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ရ' → Token 'ရ'\n",
      "  Character 'ေ' → Token 'ေ'\n",
      "  Character 'က' → Token 'က'\n",
      "  Character 'ူ' → Token 'ူ'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character 'က' → Token 'က'\n",
      "  Character 'န' → Token 'န'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character 'ီ' → Token 'ီ'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'ွ' → Token 'ွ'\n",
      "  Character 'င' → Token 'င'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ရ' → Token 'ရ'\n",
      "  Character 'ှ' → Token 'ှ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character 'ံ' → Token 'ံ'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character 'န' → Token 'န'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character '်' → Token '်'\n",
      "  Character '့' → Token '့'\n",
      "  Character 'သ' → Token 'သ'\n",
      "  Character 'ွ' → Token 'ွ'\n",
      "  Character 'ာ' → Token 'ာ'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character 'ါ' → Token 'ါ'\n",
      "  Character 'သ' → Token 'သ'\n",
      "  Character 'ည' → Token 'ည'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character '။' → Token '။'\n",
      "\n",
      "ENGLISH TOKENIZATION (Word-level):\n",
      "Original text: Italy have defeated Portugal 31-5 in Pool C of the 2007 Rugby World Cup at Parc des Princes, Paris, France.\n",
      "Tokenized into 25 tokens:\n",
      "  1. 'Italy' → ID: 1455\n",
      "  2. 'have' → ID: 27\n",
      "  3. 'defeated' → ID: 1240\n",
      "  4. 'Portugal' → ID: 2874\n",
      "  5. '31' → ID: 1421\n",
      "  6. '-' → ID: 17\n",
      "  7. '5' → ID: 141\n",
      "  8. 'in' → ID: 10\n",
      "  9. 'Pool' → ID: 7502\n",
      "  10. 'C' → ID: 908\n",
      "  11. 'of' → ID: 7\n",
      "  12. 'the' → ID: 4\n",
      "  13. '2007' → ID: 308\n",
      "  14. 'Rugby' → ID: 2625\n",
      "  15. 'World' → ID: 236\n",
      "  16. 'Cup' → ID: 565\n",
      "  17. 'at' → ID: 26\n",
      "  18. '[UNK]' → ID: 3\n",
      "  19. 'des' → ID: 9144\n",
      "  20. '[UNK]' → ID: 3\n",
      "  21. ',' → ID: 5\n",
      "  22. 'Paris' → ID: 2507\n",
      "  23. ',' → ID: 5\n",
      "  24. 'France' → ID: 395\n",
      "  25. '.' → ID: 6\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Sample 2 ===\n",
      "BURMESE TOKENIZATION (Character-level):\n",
      "Original text: အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု အဖြစ် စတုတ္ထ မိနစ် တွင် အမှတ်ပေးခြင်း ကို ဖွင့်လှစ်ပေးခဲ့သည် ။\n",
      "Tokenized into 109 tokens:\n",
      "  1. 'အ' → ID: 19\n",
      "  2. 'န' → ID: 16\n",
      "  3. '်' → ID: 5\n",
      "  4. 'ဒ' → ID: 40\n",
      "  5. 'ရ' → ID: 21\n",
      "  6. 'ီ' → ID: 29\n",
      "  7. 'ယ' → ID: 32\n",
      "  8. 'ာ' → ID: 7\n",
      "  9. ' ' → ID: 4\n",
      "  10. 'မ' → ID: 13\n",
      "  11. 'ာ' → ID: 7\n",
      "  12. 'စ' → ID: 22\n",
      "  13. 'ီ' → ID: 29\n",
      "  14. ' ' → ID: 4\n",
      "  15. 'သ' → ID: 15\n",
      "  16. 'ည' → ID: 27\n",
      "  17. '်' → ID: 5\n",
      "  18. ' ' → ID: 4\n",
      "  19. 'အ' → ID: 19\n",
      "  20. 'ီ' → ID: 29\n",
      "  21. 'တ' → ID: 14\n",
      "  22. 'လ' → ID: 25\n",
      "  23. 'ီ' → ID: 29\n",
      "  24. ' ' → ID: 4\n",
      "  25. 'အ' → ID: 19\n",
      "  26. 'တ' → ID: 14\n",
      "  27. 'ွ' → ID: 26\n",
      "  28. 'က' → ID: 9\n",
      "  29. '်' → ID: 5\n",
      "  30. ' ' → ID: 4\n",
      "  31. 'စ' → ID: 22\n",
      "  32. 'မ' → ID: 13\n",
      "  33. '်' → ID: 5\n",
      "  34. 'း' → ID: 6\n",
      "  35. 'သ' → ID: 15\n",
      "  36. 'ပ' → ID: 17\n",
      "  37. '်' → ID: 5\n",
      "  38. 'မ' → ID: 13\n",
      "  39. 'ှ' → ID: 24\n",
      "  40. 'ု' → ID: 8\n",
      "  41. ' ' → ID: 4\n",
      "  42. 'တ' → ID: 14\n",
      "  43. 'စ' → ID: 22\n",
      "  44. '်' → ID: 5\n",
      "  45. 'ခ' → ID: 23\n",
      "  46. 'ု' → ID: 8\n",
      "  47. ' ' → ID: 4\n",
      "  48. 'အ' → ID: 19\n",
      "  49. 'ဖ' → ID: 35\n",
      "  50. 'ြ' → ID: 20\n",
      "  51. 'စ' → ID: 22\n",
      "  52. '်' → ID: 5\n",
      "  53. ' ' → ID: 4\n",
      "  54. 'စ' → ID: 22\n",
      "  55. 'တ' → ID: 14\n",
      "  56. 'ု' → ID: 8\n",
      "  57. 'တ' → ID: 14\n",
      "  58. '္' → ID: 46\n",
      "  59. 'ထ' → ID: 37\n",
      "  60. ' ' → ID: 4\n",
      "  61. 'မ' → ID: 13\n",
      "  62. 'ိ' → ID: 11\n",
      "  63. 'န' → ID: 16\n",
      "  64. 'စ' → ID: 22\n",
      "  65. '်' → ID: 5\n",
      "  66. ' ' → ID: 4\n",
      "  67. 'တ' → ID: 14\n",
      "  68. 'ွ' → ID: 26\n",
      "  69. 'င' → ID: 12\n",
      "  70. '်' → ID: 5\n",
      "  71. ' ' → ID: 4\n",
      "  72. 'အ' → ID: 19\n",
      "  73. 'မ' → ID: 13\n",
      "  74. 'ှ' → ID: 24\n",
      "  75. 'တ' → ID: 14\n",
      "  76. '်' → ID: 5\n",
      "  77. 'ပ' → ID: 17\n",
      "  78. 'ေ' → ID: 10\n",
      "  79. 'း' → ID: 6\n",
      "  80. 'ခ' → ID: 23\n",
      "  81. 'ြ' → ID: 20\n",
      "  82. 'င' → ID: 12\n",
      "  83. '်' → ID: 5\n",
      "  84. 'း' → ID: 6\n",
      "  85. ' ' → ID: 4\n",
      "  86. 'က' → ID: 9\n",
      "  87. 'ိ' → ID: 11\n",
      "  88. 'ု' → ID: 8\n",
      "  89. ' ' → ID: 4\n",
      "  90. 'ဖ' → ID: 35\n",
      "  91. 'ွ' → ID: 26\n",
      "  92. 'င' → ID: 12\n",
      "  93. '်' → ID: 5\n",
      "  94. '့' → ID: 18\n",
      "  95. 'လ' → ID: 25\n",
      "  96. 'ှ' → ID: 24\n",
      "  97. 'စ' → ID: 22\n",
      "  98. '်' → ID: 5\n",
      "  99. 'ပ' → ID: 17\n",
      "  100. 'ေ' → ID: 10\n",
      "  101. 'း' → ID: 6\n",
      "  102. 'ခ' → ID: 23\n",
      "  103. 'ဲ' → ID: 30\n",
      "  104. '့' → ID: 18\n",
      "  105. 'သ' → ID: 15\n",
      "  106. 'ည' → ID: 27\n",
      "  107. '်' → ID: 5\n",
      "  108. ' ' → ID: 4\n",
      "  109. '။' → ID: 39\n",
      "\n",
      "Character by character verification:\n",
      "  Character 'အ' → Token 'အ'\n",
      "  Character 'န' → Token 'န'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'ဒ' → Token 'ဒ'\n",
      "  Character 'ရ' → Token 'ရ'\n",
      "  Character 'ီ' → Token 'ီ'\n",
      "  Character 'ယ' → Token 'ယ'\n",
      "  Character 'ာ' → Token 'ာ'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ာ' → Token 'ာ'\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character 'ီ' → Token 'ီ'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'သ' → Token 'သ'\n",
      "  Character 'ည' → Token 'ည'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'အ' → Token 'အ'\n",
      "  Character 'ီ' → Token 'ီ'\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'လ' → Token 'လ'\n",
      "  Character 'ီ' → Token 'ီ'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'အ' → Token 'အ'\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'ွ' → Token 'ွ'\n",
      "  Character 'က' → Token 'က'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character 'သ' → Token 'သ'\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ှ' → Token 'ှ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'ခ' → Token 'ခ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'အ' → Token 'အ'\n",
      "  Character 'ဖ' → Token 'ဖ'\n",
      "  Character 'ြ' → Token 'ြ'\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character '္' → Token '္'\n",
      "  Character 'ထ' → Token 'ထ'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'န' → Token 'န'\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'ွ' → Token 'ွ'\n",
      "  Character 'င' → Token 'င'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'အ' → Token 'အ'\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ှ' → Token 'ှ'\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character 'ေ' → Token 'ေ'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character 'ခ' → Token 'ခ'\n",
      "  Character 'ြ' → Token 'ြ'\n",
      "  Character 'င' → Token 'င'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'က' → Token 'က'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ဖ' → Token 'ဖ'\n",
      "  Character 'ွ' → Token 'ွ'\n",
      "  Character 'င' → Token 'င'\n",
      "  Character '်' → Token '်'\n",
      "  Character '့' → Token '့'\n",
      "  Character 'လ' → Token 'လ'\n",
      "  Character 'ှ' → Token 'ှ'\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character 'ေ' → Token 'ေ'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character 'ခ' → Token 'ခ'\n",
      "  Character 'ဲ' → Token 'ဲ'\n",
      "  Character '့' → Token '့'\n",
      "  Character 'သ' → Token 'သ'\n",
      "  Character 'ည' → Token 'ည'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character '။' → Token '။'\n",
      "\n",
      "ENGLISH TOKENIZATION (Word-level):\n",
      "Original text: Andrea Masi opened the scoring in the fourth minute with a try for Italy.\n",
      "Tokenized into 15 tokens:\n",
      "  1. 'Andrea' → ID: 6461\n",
      "  2. '[UNK]' → ID: 3\n",
      "  3. 'opened' → ID: 984\n",
      "  4. 'the' → ID: 4\n",
      "  5. 'scoring' → ID: 1487\n",
      "  6. 'in' → ID: 10\n",
      "  7. 'the' → ID: 4\n",
      "  8. 'fourth' → ID: 958\n",
      "  9. 'minute' → ID: 926\n",
      "  10. 'with' → ID: 22\n",
      "  11. 'a' → ID: 11\n",
      "  12. 'try' → ID: 821\n",
      "  13. 'for' → ID: 16\n",
      "  14. 'Italy' → ID: 1455\n",
      "  15. '.' → ID: 6\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Sample 3 ===\n",
      "BURMESE TOKENIZATION (Character-level):\n",
      "Original text: ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကို ထိန်းချုပ်ခြင်း မရှိခဲ့လျှင် အီတလီ သည် ပွဲနားချိန် မတိုင်မှီ အခြား မည်သည့် ကြိုးစားမှု များ ကိုမှ အမှတ် မရနိုင် ပေမယ့် ဒေးဗစ် ဘော်တိုလပ်စီ သည် သူတို့၏ ဦးဆောင်မှု ကို အဓွန့်ရှည် စေရန် ပယ်နယ်လ်တီ ၃ဂိုး သွင်းပေးခဲ့သည် ။\n",
      "Tokenized into 264 tokens:\n",
      "  1. 'ပ' → ID: 17\n",
      "  2. 'ထ' → ID: 37\n",
      "  3. 'မ' → ID: 13\n",
      "  4. ' ' → ID: 4\n",
      "  5. 'တ' → ID: 14\n",
      "  6. 'စ' → ID: 22\n",
      "  7. '်' → ID: 5\n",
      "  8. 'ဝ' → ID: 42\n",
      "  9. 'က' → ID: 9\n",
      "  10. '်' → ID: 5\n",
      "  11. ' ' → ID: 4\n",
      "  12. '၏' → ID: 43\n",
      "  13. ' ' → ID: 4\n",
      "  14. 'တ' → ID: 14\n",
      "  15. 'ေ' → ID: 10\n",
      "  16. 'ာ' → ID: 7\n",
      "  17. '်' → ID: 5\n",
      "  18. 'တ' → ID: 14\n",
      "  19. 'ေ' → ID: 10\n",
      "  20. 'ာ' → ID: 7\n",
      "  21. '်' → ID: 5\n",
      "  22. 'မ' → ID: 13\n",
      "  23. 'ျ' → ID: 28\n",
      "  24. 'ာ' → ID: 7\n",
      "  25. 'း' → ID: 6\n",
      "  26. 'မ' → ID: 13\n",
      "  27. 'ျ' → ID: 28\n",
      "  28. 'ာ' → ID: 7\n",
      "  29. 'း' → ID: 6\n",
      "  30. ' ' → ID: 4\n",
      "  31. 'အ' → ID: 19\n",
      "  32. 'တ' → ID: 14\n",
      "  33. 'ွ' → ID: 26\n",
      "  34. 'က' → ID: 9\n",
      "  35. '်' → ID: 5\n",
      "  36. ' ' → ID: 4\n",
      "  37. 'က' → ID: 9\n",
      "  38. 'စ' → ID: 22\n",
      "  39. 'ာ' → ID: 7\n",
      "  40. 'း' → ID: 6\n",
      "  41. 'ပ' → ID: 17\n",
      "  42. 'ွ' → ID: 26\n",
      "  43. 'ဲ' → ID: 30\n",
      "  44. ' ' → ID: 4\n",
      "  45. 'က' → ID: 9\n",
      "  46. 'ိ' → ID: 11\n",
      "  47. 'ု' → ID: 8\n",
      "  48. ' ' → ID: 4\n",
      "  49. 'ထ' → ID: 37\n",
      "  50. 'ိ' → ID: 11\n",
      "  51. 'န' → ID: 16\n",
      "  52. '်' → ID: 5\n",
      "  53. 'း' → ID: 6\n",
      "  54. 'ခ' → ID: 23\n",
      "  55. 'ျ' → ID: 28\n",
      "  56. 'ု' → ID: 8\n",
      "  57. 'ပ' → ID: 17\n",
      "  58. '်' → ID: 5\n",
      "  59. 'ခ' → ID: 23\n",
      "  60. 'ြ' → ID: 20\n",
      "  61. 'င' → ID: 12\n",
      "  62. '်' → ID: 5\n",
      "  63. 'း' → ID: 6\n",
      "  64. ' ' → ID: 4\n",
      "  65. 'မ' → ID: 13\n",
      "  66. 'ရ' → ID: 21\n",
      "  67. 'ှ' → ID: 24\n",
      "  68. 'ိ' → ID: 11\n",
      "  69. 'ခ' → ID: 23\n",
      "  70. 'ဲ' → ID: 30\n",
      "  71. '့' → ID: 18\n",
      "  72. 'လ' → ID: 25\n",
      "  73. 'ျ' → ID: 28\n",
      "  74. 'ှ' → ID: 24\n",
      "  75. 'င' → ID: 12\n",
      "  76. '်' → ID: 5\n",
      "  77. ' ' → ID: 4\n",
      "  78. 'အ' → ID: 19\n",
      "  79. 'ီ' → ID: 29\n",
      "  80. 'တ' → ID: 14\n",
      "  81. 'လ' → ID: 25\n",
      "  82. 'ီ' → ID: 29\n",
      "  83. ' ' → ID: 4\n",
      "  84. 'သ' → ID: 15\n",
      "  85. 'ည' → ID: 27\n",
      "  86. '်' → ID: 5\n",
      "  87. ' ' → ID: 4\n",
      "  88. 'ပ' → ID: 17\n",
      "  89. 'ွ' → ID: 26\n",
      "  90. 'ဲ' → ID: 30\n",
      "  91. 'န' → ID: 16\n",
      "  92. 'ာ' → ID: 7\n",
      "  93. 'း' → ID: 6\n",
      "  94. 'ခ' → ID: 23\n",
      "  95. 'ျ' → ID: 28\n",
      "  96. 'ိ' → ID: 11\n",
      "  97. 'န' → ID: 16\n",
      "  98. '်' → ID: 5\n",
      "  99. ' ' → ID: 4\n",
      "  100. 'မ' → ID: 13\n",
      "  101. 'တ' → ID: 14\n",
      "  102. 'ိ' → ID: 11\n",
      "  103. 'ု' → ID: 8\n",
      "  104. 'င' → ID: 12\n",
      "  105. '်' → ID: 5\n",
      "  106. 'မ' → ID: 13\n",
      "  107. 'ှ' → ID: 24\n",
      "  108. 'ီ' → ID: 29\n",
      "  109. ' ' → ID: 4\n",
      "  110. 'အ' → ID: 19\n",
      "  111. 'ခ' → ID: 23\n",
      "  112. 'ြ' → ID: 20\n",
      "  113. 'ာ' → ID: 7\n",
      "  114. 'း' → ID: 6\n",
      "  115. ' ' → ID: 4\n",
      "  116. 'မ' → ID: 13\n",
      "  117. 'ည' → ID: 27\n",
      "  118. '်' → ID: 5\n",
      "  119. 'သ' → ID: 15\n",
      "  120. 'ည' → ID: 27\n",
      "  121. '်' → ID: 5\n",
      "  122. '့' → ID: 18\n",
      "  123. ' ' → ID: 4\n",
      "  124. 'က' → ID: 9\n",
      "  125. 'ြ' → ID: 20\n",
      "  126. 'ိ' → ID: 11\n",
      "  127. 'ု' → ID: 8\n",
      "  128. 'း' → ID: 6\n",
      "  129. 'စ' → ID: 22\n",
      "  130. 'ာ' → ID: 7\n",
      "  131. 'း' → ID: 6\n",
      "  132. 'မ' → ID: 13\n",
      "  133. 'ှ' → ID: 24\n",
      "  134. 'ု' → ID: 8\n",
      "  135. ' ' → ID: 4\n",
      "  136. 'မ' → ID: 13\n",
      "  137. 'ျ' → ID: 28\n",
      "  138. 'ာ' → ID: 7\n",
      "  139. 'း' → ID: 6\n",
      "  140. ' ' → ID: 4\n",
      "  141. 'က' → ID: 9\n",
      "  142. 'ိ' → ID: 11\n",
      "  143. 'ု' → ID: 8\n",
      "  144. 'မ' → ID: 13\n",
      "  145. 'ှ' → ID: 24\n",
      "  146. ' ' → ID: 4\n",
      "  147. 'အ' → ID: 19\n",
      "  148. 'မ' → ID: 13\n",
      "  149. 'ှ' → ID: 24\n",
      "  150. 'တ' → ID: 14\n",
      "  151. '်' → ID: 5\n",
      "  152. ' ' → ID: 4\n",
      "  153. 'မ' → ID: 13\n",
      "  154. 'ရ' → ID: 21\n",
      "  155. 'န' → ID: 16\n",
      "  156. 'ိ' → ID: 11\n",
      "  157. 'ု' → ID: 8\n",
      "  158. 'င' → ID: 12\n",
      "  159. '်' → ID: 5\n",
      "  160. ' ' → ID: 4\n",
      "  161. 'ပ' → ID: 17\n",
      "  162. 'ေ' → ID: 10\n",
      "  163. 'မ' → ID: 13\n",
      "  164. 'ယ' → ID: 32\n",
      "  165. '်' → ID: 5\n",
      "  166. '့' → ID: 18\n",
      "  167. ' ' → ID: 4\n",
      "  168. 'ဒ' → ID: 40\n",
      "  169. 'ေ' → ID: 10\n",
      "  170. 'း' → ID: 6\n",
      "  171. 'ဗ' → ID: 48\n",
      "  172. 'စ' → ID: 22\n",
      "  173. '်' → ID: 5\n",
      "  174. ' ' → ID: 4\n",
      "  175. 'ဘ' → ID: 41\n",
      "  176. 'ေ' → ID: 10\n",
      "  177. 'ာ' → ID: 7\n",
      "  178. '်' → ID: 5\n",
      "  179. 'တ' → ID: 14\n",
      "  180. 'ိ' → ID: 11\n",
      "  181. 'ု' → ID: 8\n",
      "  182. 'လ' → ID: 25\n",
      "  183. 'ပ' → ID: 17\n",
      "  184. '်' → ID: 5\n",
      "  185. 'စ' → ID: 22\n",
      "  186. 'ီ' → ID: 29\n",
      "  187. ' ' → ID: 4\n",
      "  188. 'သ' → ID: 15\n",
      "  189. 'ည' → ID: 27\n",
      "  190. '်' → ID: 5\n",
      "  191. ' ' → ID: 4\n",
      "  192. 'သ' → ID: 15\n",
      "  193. 'ူ' → ID: 34\n",
      "  194. 'တ' → ID: 14\n",
      "  195. 'ိ' → ID: 11\n",
      "  196. 'ု' → ID: 8\n",
      "  197. '့' → ID: 18\n",
      "  198. '၏' → ID: 43\n",
      "  199. ' ' → ID: 4\n",
      "  200. 'ဦ' → ID: 59\n",
      "  201. 'း' → ID: 6\n",
      "  202. 'ဆ' → ID: 31\n",
      "  203. 'ေ' → ID: 10\n",
      "  204. 'ာ' → ID: 7\n",
      "  205. 'င' → ID: 12\n",
      "  206. '်' → ID: 5\n",
      "  207. 'မ' → ID: 13\n",
      "  208. 'ှ' → ID: 24\n",
      "  209. 'ု' → ID: 8\n",
      "  210. ' ' → ID: 4\n",
      "  211. 'က' → ID: 9\n",
      "  212. 'ိ' → ID: 11\n",
      "  213. 'ု' → ID: 8\n",
      "  214. ' ' → ID: 4\n",
      "  215. 'အ' → ID: 19\n",
      "  216. 'ဓ' → ID: 66\n",
      "  217. 'ွ' → ID: 26\n",
      "  218. 'န' → ID: 16\n",
      "  219. '်' → ID: 5\n",
      "  220. '့' → ID: 18\n",
      "  221. 'ရ' → ID: 21\n",
      "  222. 'ှ' → ID: 24\n",
      "  223. 'ည' → ID: 27\n",
      "  224. '်' → ID: 5\n",
      "  225. ' ' → ID: 4\n",
      "  226. 'စ' → ID: 22\n",
      "  227. 'ေ' → ID: 10\n",
      "  228. 'ရ' → ID: 21\n",
      "  229. 'န' → ID: 16\n",
      "  230. '်' → ID: 5\n",
      "  231. ' ' → ID: 4\n",
      "  232. 'ပ' → ID: 17\n",
      "  233. 'ယ' → ID: 32\n",
      "  234. '်' → ID: 5\n",
      "  235. 'န' → ID: 16\n",
      "  236. 'ယ' → ID: 32\n",
      "  237. '်' → ID: 5\n",
      "  238. 'လ' → ID: 25\n",
      "  239. '်' → ID: 5\n",
      "  240. 'တ' → ID: 14\n",
      "  241. 'ီ' → ID: 29\n",
      "  242. ' ' → ID: 4\n",
      "  243. '၃' → ID: 64\n",
      "  244. 'ဂ' → ID: 45\n",
      "  245. 'ိ' → ID: 11\n",
      "  246. 'ု' → ID: 8\n",
      "  247. 'း' → ID: 6\n",
      "  248. ' ' → ID: 4\n",
      "  249. 'သ' → ID: 15\n",
      "  250. 'ွ' → ID: 26\n",
      "  251. 'င' → ID: 12\n",
      "  252. '်' → ID: 5\n",
      "  253. 'း' → ID: 6\n",
      "  254. 'ပ' → ID: 17\n",
      "  255. 'ေ' → ID: 10\n",
      "  256. 'း' → ID: 6\n",
      "  257. 'ခ' → ID: 23\n",
      "  258. 'ဲ' → ID: 30\n",
      "  259. '့' → ID: 18\n",
      "  260. 'သ' → ID: 15\n",
      "  261. 'ည' → ID: 27\n",
      "  262. '်' → ID: 5\n",
      "  263. ' ' → ID: 4\n",
      "  264. '။' → ID: 39\n",
      "\n",
      "Character by character verification:\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character 'ထ' → Token 'ထ'\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'ဝ' → Token 'ဝ'\n",
      "  Character 'က' → Token 'က'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character '၏' → Token '၏'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'ေ' → Token 'ေ'\n",
      "  Character 'ာ' → Token 'ာ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'ေ' → Token 'ေ'\n",
      "  Character 'ာ' → Token 'ာ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ျ' → Token 'ျ'\n",
      "  Character 'ာ' → Token 'ာ'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ျ' → Token 'ျ'\n",
      "  Character 'ာ' → Token 'ာ'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'အ' → Token 'အ'\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'ွ' → Token 'ွ'\n",
      "  Character 'က' → Token 'က'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'က' → Token 'က'\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character 'ာ' → Token 'ာ'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character 'ွ' → Token 'ွ'\n",
      "  Character 'ဲ' → Token 'ဲ'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'က' → Token 'က'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ထ' → Token 'ထ'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'န' → Token 'န'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character 'ခ' → Token 'ခ'\n",
      "  Character 'ျ' → Token 'ျ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'ခ' → Token 'ခ'\n",
      "  Character 'ြ' → Token 'ြ'\n",
      "  Character 'င' → Token 'င'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ရ' → Token 'ရ'\n",
      "  Character 'ှ' → Token 'ှ'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'ခ' → Token 'ခ'\n",
      "  Character 'ဲ' → Token 'ဲ'\n",
      "  Character '့' → Token '့'\n",
      "  Character 'လ' → Token 'လ'\n",
      "  Character 'ျ' → Token 'ျ'\n",
      "  Character 'ှ' → Token 'ှ'\n",
      "  Character 'င' → Token 'င'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'အ' → Token 'အ'\n",
      "  Character 'ီ' → Token 'ီ'\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'လ' → Token 'လ'\n",
      "  Character 'ီ' → Token 'ီ'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'သ' → Token 'သ'\n",
      "  Character 'ည' → Token 'ည'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character 'ွ' → Token 'ွ'\n",
      "  Character 'ဲ' → Token 'ဲ'\n",
      "  Character 'န' → Token 'န'\n",
      "  Character 'ာ' → Token 'ာ'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character 'ခ' → Token 'ခ'\n",
      "  Character 'ျ' → Token 'ျ'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'န' → Token 'န'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character 'င' → Token 'င'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ှ' → Token 'ှ'\n",
      "  Character 'ီ' → Token 'ီ'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'အ' → Token 'အ'\n",
      "  Character 'ခ' → Token 'ခ'\n",
      "  Character 'ြ' → Token 'ြ'\n",
      "  Character 'ာ' → Token 'ာ'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ည' → Token 'ည'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'သ' → Token 'သ'\n",
      "  Character 'ည' → Token 'ည'\n",
      "  Character '်' → Token '်'\n",
      "  Character '့' → Token '့'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'က' → Token 'က'\n",
      "  Character 'ြ' → Token 'ြ'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character 'ာ' → Token 'ာ'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ှ' → Token 'ှ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ျ' → Token 'ျ'\n",
      "  Character 'ာ' → Token 'ာ'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'က' → Token 'က'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ှ' → Token 'ှ'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'အ' → Token 'အ'\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ှ' → Token 'ှ'\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ရ' → Token 'ရ'\n",
      "  Character 'န' → Token 'န'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character 'င' → Token 'င'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character 'ေ' → Token 'ေ'\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ယ' → Token 'ယ'\n",
      "  Character '်' → Token '်'\n",
      "  Character '့' → Token '့'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ဒ' → Token 'ဒ'\n",
      "  Character 'ေ' → Token 'ေ'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character 'ဗ' → Token 'ဗ'\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ဘ' → Token 'ဘ'\n",
      "  Character 'ေ' → Token 'ေ'\n",
      "  Character 'ာ' → Token 'ာ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character 'လ' → Token 'လ'\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character 'ီ' → Token 'ီ'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'သ' → Token 'သ'\n",
      "  Character 'ည' → Token 'ည'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'သ' → Token 'သ'\n",
      "  Character 'ူ' → Token 'ူ'\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character '့' → Token '့'\n",
      "  Character '၏' → Token '၏'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ဦ' → Token 'ဦ'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character 'ဆ' → Token 'ဆ'\n",
      "  Character 'ေ' → Token 'ေ'\n",
      "  Character 'ာ' → Token 'ာ'\n",
      "  Character 'င' → Token 'င'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'မ' → Token 'မ'\n",
      "  Character 'ှ' → Token 'ှ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'က' → Token 'က'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'အ' → Token 'အ'\n",
      "  Character 'ဓ' → Token 'ဓ'\n",
      "  Character 'ွ' → Token 'ွ'\n",
      "  Character 'န' → Token 'န'\n",
      "  Character '်' → Token '်'\n",
      "  Character '့' → Token '့'\n",
      "  Character 'ရ' → Token 'ရ'\n",
      "  Character 'ှ' → Token 'ှ'\n",
      "  Character 'ည' → Token 'ည'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'စ' → Token 'စ'\n",
      "  Character 'ေ' → Token 'ေ'\n",
      "  Character 'ရ' → Token 'ရ'\n",
      "  Character 'န' → Token 'န'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character 'ယ' → Token 'ယ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'န' → Token 'န'\n",
      "  Character 'ယ' → Token 'ယ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'လ' → Token 'လ'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'တ' → Token 'တ'\n",
      "  Character 'ီ' → Token 'ီ'\n",
      "  Character ' ' → Token ' '\n",
      "  Character '၃' → Token '၃'\n",
      "  Character 'ဂ' → Token 'ဂ'\n",
      "  Character 'ိ' → Token 'ိ'\n",
      "  Character 'ု' → Token 'ု'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character ' ' → Token ' '\n",
      "  Character 'သ' → Token 'သ'\n",
      "  Character 'ွ' → Token 'ွ'\n",
      "  Character 'င' → Token 'င'\n",
      "  Character '်' → Token '်'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character 'ပ' → Token 'ပ'\n",
      "  Character 'ေ' → Token 'ေ'\n",
      "  Character 'း' → Token 'း'\n",
      "  Character 'ခ' → Token 'ခ'\n",
      "  Character 'ဲ' → Token 'ဲ'\n",
      "  Character '့' → Token '့'\n",
      "  Character 'သ' → Token 'သ'\n",
      "  Character 'ည' → Token 'ည'\n",
      "  Character '်' → Token '်'\n",
      "  Character ' ' → Token ' '\n",
      "  Character '။' → Token '။'\n",
      "\n",
      "ENGLISH TOKENIZATION (Word-level):\n",
      "Original text: Despite controlling the game for much of the first half, Italy could not score any other tries before the interval but David Bortolussi kicked three penalties to extend their lead.\n",
      "Tokenized into 32 tokens:\n",
      "  1. 'Despite' → ID: 1203\n",
      "  2. 'controlling' → ID: 4628\n",
      "  3. 'the' → ID: 4\n",
      "  4. 'game' → ID: 229\n",
      "  5. 'for' → ID: 16\n",
      "  6. 'much' → ID: 320\n",
      "  7. 'of' → ID: 7\n",
      "  8. 'the' → ID: 4\n",
      "  9. 'first' → ID: 64\n",
      "  10. 'half' → ID: 255\n",
      "  11. ',' → ID: 5\n",
      "  12. 'Italy' → ID: 1455\n",
      "  13. 'could' → ID: 100\n",
      "  14. 'not' → ID: 37\n",
      "  15. 'score' → ID: 1320\n",
      "  16. 'any' → ID: 127\n",
      "  17. 'other' → ID: 78\n",
      "  18. 'tries' → ID: 3667\n",
      "  19. 'before' → ID: 107\n",
      "  20. 'the' → ID: 4\n",
      "  21. 'interval' → ID: 7922\n",
      "  22. 'but' → ID: 49\n",
      "  23. 'David' → ID: 571\n",
      "  24. '[UNK]' → ID: 3\n",
      "  25. 'kicked' → ID: 3611\n",
      "  26. 'three' → ID: 96\n",
      "  27. 'penalties' → ID: 3624\n",
      "  28. 'to' → ID: 8\n",
      "  29. 'extend' → ID: 2652\n",
      "  30. 'their' → ID: 45\n",
      "  31. 'lead' → ID: 296\n",
      "  32. '.' → ID: 6\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Create tokenizers\n",
    "src_tokenizer = create_burmese_tokenizer(train_dataset, config)\n",
    "tgt_tokenizer = create_english_tokenizer(train_dataset, config)\n",
    "\n",
    "# Function to inspect tokenization\n",
    "def inspect_tokenization(text, tokenizer, is_character_level=True):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    tokens = encoded.tokens\n",
    "    ids = encoded.ids\n",
    "    \n",
    "    print(f\"Original text: {text}\")\n",
    "    print(f\"Tokenized into {len(tokens)} tokens:\")\n",
    "    \n",
    "    # Print each token and its ID\n",
    "    for i, (token, id) in enumerate(zip(tokens, ids)):\n",
    "        print(f\"  {i+1}. '{token}' → ID: {id}\")\n",
    "    \n",
    "    # For character tokenization, verify each character\n",
    "    if is_character_level:\n",
    "        print(\"\\nCharacter by character verification:\")\n",
    "        for i, char in enumerate(text):\n",
    "            if i < len(tokens):\n",
    "                print(f\"  Character '{char}' → Token '{tokens[i]}'\")\n",
    "            else:\n",
    "                print(f\"  Character '{char}' → Not tokenized!\")\n",
    "\n",
    "# Inspect a few examples\n",
    "burmese_samples = [train_dataset['translation'][i]['burmese'] for i in range(3)]\n",
    "english_samples = [train_dataset['translation'][i]['english'] for i in range(3)]\n",
    "\n",
    "for i, (bur, eng) in enumerate(zip(burmese_samples, english_samples)):\n",
    "    print(f\"\\n=== Sample {i+1} ===\")\n",
    "    print(\"BURMESE TOKENIZATION (Character-level):\")\n",
    "    inspect_tokenization(bur, src_tokenizer, is_character_level=True)\n",
    "    print(\"\\nENGLISH TOKENIZATION (Word-level):\")\n",
    "    inspect_tokenization(eng, tgt_tokenizer, is_character_level=False)\n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂၀၀၇ခုနှစ် ရပ်ဘီ ကမ္ဘာ့ ဖလား တွင် အီတလီ သည် ပေါ်တူဂီ ကို ၃၁-၅ ဂိုး ဖြင့် ရေကူးကန် စီ တွင် ရှုံးနိမ့်သွားပါသည် ။ Italy have defeated Portugal 31-5 in Pool C of the 2007 Rugby World Cup at Parc des Princes, Paris, France.\n"
     ]
    }
   ],
   "source": [
    "for item in train_dataset['translation']:\n",
    "    print(item['burmese'],item['english'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burmese max length: 826\n",
      "English max length: 95\n",
      "Burmese 95th percentile: 348.0\n",
      "English 95th percentile: 41.0\n"
     ]
    }
   ],
   "source": [
    "burmese_lengths = [len(list(text['burmese'])) for text in train_dataset[\"translation\"]] \n",
    "english_lengths = [len(text['english'].split()) for text in train_dataset[\"translation\"]]  # For word-level\n",
    "\n",
    "burmese_max_length=0\n",
    "english_max_length=0\n",
    "for i in range(len(burmese_lengths)):\n",
    "    burmese_max_length=max(burmese_max_length,burmese_lengths[i])\n",
    "for j in range(len(english_lengths)):\n",
    "    english_max_length=max(english_max_length,english_lengths[j])\n",
    "\n",
    "print(f\"Burmese max length: {burmese_max_length}\")\n",
    "print(f\"English max length: {english_max_length}\")\n",
    "\n",
    "import numpy as np\n",
    "print(f\"Burmese 95th percentile: {np.percentile(burmese_lengths, 95)}\")\n",
    "print(f\"English 95th percentile: {np.percentile(english_lengths, 95)}\")\n",
    "\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No empty Burmese texts found\n"
     ]
    }
   ],
   "source": [
    "empty_texts = [i for i, text in enumerate(train_dataset[\"translation\"]) if len(text['burmese']) == 0]\n",
    "\n",
    "if empty_texts:\n",
    "    print(f\"Found {len(empty_texts)} empty Burmese texts at indices: {empty_texts[:10]}...\")\n",
    "else:\n",
    "    print(\"No empty Burmese texts found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
